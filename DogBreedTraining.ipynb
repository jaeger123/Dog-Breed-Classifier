{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Download the dog breed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogImages/test:\r\n",
      "001.Affenpinscher\t\t    068.Flat-coated_retriever\r\n",
      "002.Afghan_hound\t\t    069.French_bulldog\r\n",
      "003.Airedale_terrier\t\t    070.German_pinscher\r\n",
      "004.Akita\t\t\t    071.German_shepherd_dog\r\n",
      "005.Alaskan_malamute\t\t    072.German_shorthaired_pointer\r\n",
      "006.American_eskimo_dog\t\t    073.German_wirehaired_pointer\r\n",
      "007.American_foxhound\t\t    074.Giant_schnauzer\r\n",
      "008.American_staffordshire_terrier  075.Glen_of_imaal_terrier\r\n",
      "009.American_water_spaniel\t    076.Golden_retriever\r\n",
      "010.Anatolian_shepherd_dog\t    077.Gordon_setter\r\n",
      "011.Australian_cattle_dog\t    078.Great_dane\r\n",
      "012.Australian_shepherd\t\t    079.Great_pyrenees\r\n",
      "013.Australian_terrier\t\t    080.Greater_swiss_mountain_dog\r\n",
      "014.Basenji\t\t\t    081.Greyhound\r\n",
      "015.Basset_hound\t\t    082.Havanese\r\n",
      "016.Beagle\t\t\t    083.Ibizan_hound\r\n",
      "017.Bearded_collie\t\t    084.Icelandic_sheepdog\r\n",
      "018.Beauceron\t\t\t    085.Irish_red_and_white_setter\r\n",
      "019.Bedlington_terrier\t\t    086.Irish_setter\r\n",
      "020.Belgian_malinois\t\t    087.Irish_terrier\r\n",
      "021.Belgian_sheepdog\t\t    088.Irish_water_spaniel\r\n",
      "022.Belgian_tervuren\t\t    089.Irish_wolfhound\r\n",
      "023.Bernese_mountain_dog\t    090.Italian_greyhound\r\n",
      "024.Bichon_frise\t\t    091.Japanese_chin\r\n",
      "025.Black_and_tan_coonhound\t    092.Keeshond\r\n",
      "026.Black_russian_terrier\t    093.Kerry_blue_terrier\r\n",
      "027.Bloodhound\t\t\t    094.Komondor\r\n",
      "028.Bluetick_coonhound\t\t    095.Kuvasz\r\n",
      "029.Border_collie\t\t    096.Labrador_retriever\r\n",
      "030.Border_terrier\t\t    097.Lakeland_terrier\r\n",
      "031.Borzoi\t\t\t    098.Leonberger\r\n",
      "032.Boston_terrier\t\t    099.Lhasa_apso\r\n",
      "033.Bouvier_des_flandres\t    100.Lowchen\r\n",
      "034.Boxer\t\t\t    101.Maltese\r\n",
      "035.Boykin_spaniel\t\t    102.Manchester_terrier\r\n",
      "036.Briard\t\t\t    103.Mastiff\r\n",
      "037.Brittany\t\t\t    104.Miniature_schnauzer\r\n",
      "038.Brussels_griffon\t\t    105.Neapolitan_mastiff\r\n",
      "039.Bull_terrier\t\t    106.Newfoundland\r\n",
      "040.Bulldog\t\t\t    107.Norfolk_terrier\r\n",
      "041.Bullmastiff\t\t\t    108.Norwegian_buhund\r\n",
      "042.Cairn_terrier\t\t    109.Norwegian_elkhound\r\n",
      "043.Canaan_dog\t\t\t    110.Norwegian_lundehund\r\n",
      "044.Cane_corso\t\t\t    111.Norwich_terrier\r\n",
      "045.Cardigan_welsh_corgi\t    112.Nova_scotia_duck_tolling_retriever\r\n",
      "046.Cavalier_king_charles_spaniel   113.Old_english_sheepdog\r\n",
      "047.Chesapeake_bay_retriever\t    114.Otterhound\r\n",
      "048.Chihuahua\t\t\t    115.Papillon\r\n",
      "049.Chinese_crested\t\t    116.Parson_russell_terrier\r\n",
      "050.Chinese_shar-pei\t\t    117.Pekingese\r\n",
      "051.Chow_chow\t\t\t    118.Pembroke_welsh_corgi\r\n",
      "052.Clumber_spaniel\t\t    119.Petit_basset_griffon_vendeen\r\n",
      "053.Cocker_spaniel\t\t    120.Pharaoh_hound\r\n",
      "054.Collie\t\t\t    121.Plott\r\n",
      "055.Curly-coated_retriever\t    122.Pointer\r\n",
      "056.Dachshund\t\t\t    123.Pomeranian\r\n",
      "057.Dalmatian\t\t\t    124.Poodle\r\n",
      "058.Dandie_dinmont_terrier\t    125.Portuguese_water_dog\r\n",
      "059.Doberman_pinscher\t\t    126.Saint_bernard\r\n",
      "060.Dogue_de_bordeaux\t\t    127.Silky_terrier\r\n",
      "061.English_cocker_spaniel\t    128.Smooth_fox_terrier\r\n",
      "062.English_setter\t\t    129.Tibetan_mastiff\r\n",
      "063.English_springer_spaniel\t    130.Welsh_springer_spaniel\r\n",
      "064.English_toy_spaniel\t\t    131.Wirehaired_pointing_griffon\r\n",
      "065.Entlebucher_mountain_dog\t    132.Xoloitzcuintli\r\n",
      "066.Field_spaniel\t\t    133.Yorkshire_terrier\r\n",
      "067.Finnish_spitz\r\n",
      "\r\n",
      "dogImages/train:\r\n",
      "001.Affenpinscher\t\t    068.Flat-coated_retriever\r\n",
      "002.Afghan_hound\t\t    069.French_bulldog\r\n",
      "003.Airedale_terrier\t\t    070.German_pinscher\r\n",
      "004.Akita\t\t\t    071.German_shepherd_dog\r\n",
      "005.Alaskan_malamute\t\t    072.German_shorthaired_pointer\r\n",
      "006.American_eskimo_dog\t\t    073.German_wirehaired_pointer\r\n",
      "007.American_foxhound\t\t    074.Giant_schnauzer\r\n",
      "008.American_staffordshire_terrier  075.Glen_of_imaal_terrier\r\n",
      "009.American_water_spaniel\t    076.Golden_retriever\r\n",
      "010.Anatolian_shepherd_dog\t    077.Gordon_setter\r\n",
      "011.Australian_cattle_dog\t    078.Great_dane\r\n",
      "012.Australian_shepherd\t\t    079.Great_pyrenees\r\n",
      "013.Australian_terrier\t\t    080.Greater_swiss_mountain_dog\r\n",
      "014.Basenji\t\t\t    081.Greyhound\r\n",
      "015.Basset_hound\t\t    082.Havanese\r\n",
      "016.Beagle\t\t\t    083.Ibizan_hound\r\n",
      "017.Bearded_collie\t\t    084.Icelandic_sheepdog\r\n",
      "018.Beauceron\t\t\t    085.Irish_red_and_white_setter\r\n",
      "019.Bedlington_terrier\t\t    086.Irish_setter\r\n",
      "020.Belgian_malinois\t\t    087.Irish_terrier\r\n",
      "021.Belgian_sheepdog\t\t    088.Irish_water_spaniel\r\n",
      "022.Belgian_tervuren\t\t    089.Irish_wolfhound\r\n",
      "023.Bernese_mountain_dog\t    090.Italian_greyhound\r\n",
      "024.Bichon_frise\t\t    091.Japanese_chin\r\n",
      "025.Black_and_tan_coonhound\t    092.Keeshond\r\n",
      "026.Black_russian_terrier\t    093.Kerry_blue_terrier\r\n",
      "027.Bloodhound\t\t\t    094.Komondor\r\n",
      "028.Bluetick_coonhound\t\t    095.Kuvasz\r\n",
      "029.Border_collie\t\t    096.Labrador_retriever\r\n",
      "030.Border_terrier\t\t    097.Lakeland_terrier\r\n",
      "031.Borzoi\t\t\t    098.Leonberger\r\n",
      "032.Boston_terrier\t\t    099.Lhasa_apso\r\n",
      "033.Bouvier_des_flandres\t    100.Lowchen\r\n",
      "034.Boxer\t\t\t    101.Maltese\r\n",
      "035.Boykin_spaniel\t\t    102.Manchester_terrier\r\n",
      "036.Briard\t\t\t    103.Mastiff\r\n",
      "037.Brittany\t\t\t    104.Miniature_schnauzer\r\n",
      "038.Brussels_griffon\t\t    105.Neapolitan_mastiff\r\n",
      "039.Bull_terrier\t\t    106.Newfoundland\r\n",
      "040.Bulldog\t\t\t    107.Norfolk_terrier\r\n",
      "041.Bullmastiff\t\t\t    108.Norwegian_buhund\r\n",
      "042.Cairn_terrier\t\t    109.Norwegian_elkhound\r\n",
      "043.Canaan_dog\t\t\t    110.Norwegian_lundehund\r\n",
      "044.Cane_corso\t\t\t    111.Norwich_terrier\r\n",
      "045.Cardigan_welsh_corgi\t    112.Nova_scotia_duck_tolling_retriever\r\n",
      "046.Cavalier_king_charles_spaniel   113.Old_english_sheepdog\r\n",
      "047.Chesapeake_bay_retriever\t    114.Otterhound\r\n",
      "048.Chihuahua\t\t\t    115.Papillon\r\n",
      "049.Chinese_crested\t\t    116.Parson_russell_terrier\r\n",
      "050.Chinese_shar-pei\t\t    117.Pekingese\r\n",
      "051.Chow_chow\t\t\t    118.Pembroke_welsh_corgi\r\n",
      "052.Clumber_spaniel\t\t    119.Petit_basset_griffon_vendeen\r\n",
      "053.Cocker_spaniel\t\t    120.Pharaoh_hound\r\n",
      "054.Collie\t\t\t    121.Plott\r\n",
      "055.Curly-coated_retriever\t    122.Pointer\r\n",
      "056.Dachshund\t\t\t    123.Pomeranian\r\n",
      "057.Dalmatian\t\t\t    124.Poodle\r\n",
      "058.Dandie_dinmont_terrier\t    125.Portuguese_water_dog\r\n",
      "059.Doberman_pinscher\t\t    126.Saint_bernard\r\n",
      "060.Dogue_de_bordeaux\t\t    127.Silky_terrier\r\n",
      "061.English_cocker_spaniel\t    128.Smooth_fox_terrier\r\n",
      "062.English_setter\t\t    129.Tibetan_mastiff\r\n",
      "063.English_springer_spaniel\t    130.Welsh_springer_spaniel\r\n",
      "064.English_toy_spaniel\t\t    131.Wirehaired_pointing_griffon\r\n",
      "065.Entlebucher_mountain_dog\t    132.Xoloitzcuintli\r\n",
      "066.Field_spaniel\t\t    133.Yorkshire_terrier\r\n",
      "067.Finnish_spitz\r\n",
      "\r\n",
      "dogImages/valid:\r\n",
      "001.Affenpinscher\t\t    068.Flat-coated_retriever\r\n",
      "002.Afghan_hound\t\t    069.French_bulldog\r\n",
      "003.Airedale_terrier\t\t    070.German_pinscher\r\n",
      "004.Akita\t\t\t    071.German_shepherd_dog\r\n",
      "005.Alaskan_malamute\t\t    072.German_shorthaired_pointer\r\n",
      "006.American_eskimo_dog\t\t    073.German_wirehaired_pointer\r\n",
      "007.American_foxhound\t\t    074.Giant_schnauzer\r\n",
      "008.American_staffordshire_terrier  075.Glen_of_imaal_terrier\r\n",
      "009.American_water_spaniel\t    076.Golden_retriever\r\n",
      "010.Anatolian_shepherd_dog\t    077.Gordon_setter\r\n",
      "011.Australian_cattle_dog\t    078.Great_dane\r\n",
      "012.Australian_shepherd\t\t    079.Great_pyrenees\r\n",
      "013.Australian_terrier\t\t    080.Greater_swiss_mountain_dog\r\n",
      "014.Basenji\t\t\t    081.Greyhound\r\n",
      "015.Basset_hound\t\t    082.Havanese\r\n",
      "016.Beagle\t\t\t    083.Ibizan_hound\r\n",
      "017.Bearded_collie\t\t    084.Icelandic_sheepdog\r\n",
      "018.Beauceron\t\t\t    085.Irish_red_and_white_setter\r\n",
      "019.Bedlington_terrier\t\t    086.Irish_setter\r\n",
      "020.Belgian_malinois\t\t    087.Irish_terrier\r\n",
      "021.Belgian_sheepdog\t\t    088.Irish_water_spaniel\r\n",
      "022.Belgian_tervuren\t\t    089.Irish_wolfhound\r\n",
      "023.Bernese_mountain_dog\t    090.Italian_greyhound\r\n",
      "024.Bichon_frise\t\t    091.Japanese_chin\r\n",
      "025.Black_and_tan_coonhound\t    092.Keeshond\r\n",
      "026.Black_russian_terrier\t    093.Kerry_blue_terrier\r\n",
      "027.Bloodhound\t\t\t    094.Komondor\r\n",
      "028.Bluetick_coonhound\t\t    095.Kuvasz\r\n",
      "029.Border_collie\t\t    096.Labrador_retriever\r\n",
      "030.Border_terrier\t\t    097.Lakeland_terrier\r\n",
      "031.Borzoi\t\t\t    098.Leonberger\r\n",
      "032.Boston_terrier\t\t    099.Lhasa_apso\r\n",
      "033.Bouvier_des_flandres\t    100.Lowchen\r\n",
      "034.Boxer\t\t\t    101.Maltese\r\n",
      "035.Boykin_spaniel\t\t    102.Manchester_terrier\r\n",
      "036.Briard\t\t\t    103.Mastiff\r\n",
      "037.Brittany\t\t\t    104.Miniature_schnauzer\r\n",
      "038.Brussels_griffon\t\t    105.Neapolitan_mastiff\r\n",
      "039.Bull_terrier\t\t    106.Newfoundland\r\n",
      "040.Bulldog\t\t\t    107.Norfolk_terrier\r\n",
      "041.Bullmastiff\t\t\t    108.Norwegian_buhund\r\n",
      "042.Cairn_terrier\t\t    109.Norwegian_elkhound\r\n",
      "043.Canaan_dog\t\t\t    110.Norwegian_lundehund\r\n",
      "044.Cane_corso\t\t\t    111.Norwich_terrier\r\n",
      "045.Cardigan_welsh_corgi\t    112.Nova_scotia_duck_tolling_retriever\r\n",
      "046.Cavalier_king_charles_spaniel   113.Old_english_sheepdog\r\n",
      "047.Chesapeake_bay_retriever\t    114.Otterhound\r\n",
      "048.Chihuahua\t\t\t    115.Papillon\r\n",
      "049.Chinese_crested\t\t    116.Parson_russell_terrier\r\n",
      "050.Chinese_shar-pei\t\t    117.Pekingese\r\n",
      "051.Chow_chow\t\t\t    118.Pembroke_welsh_corgi\r\n",
      "052.Clumber_spaniel\t\t    119.Petit_basset_griffon_vendeen\r\n",
      "053.Cocker_spaniel\t\t    120.Pharaoh_hound\r\n",
      "054.Collie\t\t\t    121.Plott\r\n",
      "055.Curly-coated_retriever\t    122.Pointer\r\n",
      "056.Dachshund\t\t\t    123.Pomeranian\r\n",
      "057.Dalmatian\t\t\t    124.Poodle\r\n",
      "058.Dandie_dinmont_terrier\t    125.Portuguese_water_dog\r\n",
      "059.Doberman_pinscher\t\t    126.Saint_bernard\r\n",
      "060.Dogue_de_bordeaux\t\t    127.Silky_terrier\r\n",
      "061.English_cocker_spaniel\t    128.Smooth_fox_terrier\r\n",
      "062.English_setter\t\t    129.Tibetan_mastiff\r\n",
      "063.English_springer_spaniel\t    130.Welsh_springer_spaniel\r\n",
      "064.English_toy_spaniel\t\t    131.Wirehaired_pointing_griffon\r\n",
      "065.Entlebucher_mountain_dog\t    132.Xoloitzcuintli\r\n",
      "066.Field_spaniel\t\t    133.Yorkshire_terrier\r\n",
      "067.Finnish_spitz\r\n"
     ]
    }
   ],
   "source": [
    "!ls dogImages/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Clearly from the folders names, we can see that there are 133 dog breed classes in our dataset. Thus our task it to train a 133 class Deep Learning CNN model which can classify the dogs on the basis of their breed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages, and dog breed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numrical comp\n",
    "import numpy as np\n",
    "# reading dir\n",
    "from glob import glob\n",
    "# cv\n",
    "import cv2     \n",
    "import os\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline \n",
    "# printing loop time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import vgg16, resnet101\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8351 total dog images.\n"
     ]
    }
   ],
   "source": [
    "dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
    "\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Exploration and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write transforms and data loaders for training, testing and validation\n",
    "\n",
    "The 224 X 224 random crop from every image for training. For testing and validation, the images are first resizrd to 256 X 256 center crop will be taken.  As a part of data-augmentation to the training set, we will use, random rotation, and horizontal flipping. Also, before passing the image as an input to the CNN model, we will normalize the images by subtracting mean, and dividing it by standard deviation. This is done for the three RGB channels separately. Channel-wise mean and standard deviation values used for normalisation are from large scale Imagenet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dogImages/'\n",
    "\n",
    "data_transforms_train =  transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_transforms_valid = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_transforms_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# create a dictionary of three Imagefolder objects\n",
    "train_data =  datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms_train)\n",
    "test_data =  datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms_test)\n",
    "val_data =  datasets.ImageFolder(os.path.join(data_dir, 'valid'), data_transforms_valid)\n",
    "\n",
    "# create dataloaders, there is no point in shuffling the testing and validation data\n",
    "loaders = {}\n",
    "loaders['train'] = DataLoader(train_data, batch_size=256, shuffle=True, drop_last=True)\n",
    "loaders['test'] = DataLoader(test_data, batch_size=128, shuffle=False, drop_last=True)\n",
    "loaders['val'] = DataLoader(val_data, batch_size=128, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check dog breed classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 classes in the dataset\n",
      "\n",
      "001.Affenpinscher\n",
      "002.Afghan_hound\n",
      "003.Airedale_terrier\n",
      "004.Akita\n",
      "005.Alaskan_malamute\n",
      "006.American_eskimo_dog\n",
      "007.American_foxhound\n",
      "008.American_staffordshire_terrier\n",
      "009.American_water_spaniel\n",
      "010.Anatolian_shepherd_dog\n",
      "011.Australian_cattle_dog\n",
      "012.Australian_shepherd\n",
      "013.Australian_terrier\n",
      "014.Basenji\n",
      "015.Basset_hound\n",
      "016.Beagle\n",
      "017.Bearded_collie\n",
      "018.Beauceron\n",
      "019.Bedlington_terrier\n",
      "020.Belgian_malinois\n",
      "021.Belgian_sheepdog\n",
      "022.Belgian_tervuren\n",
      "023.Bernese_mountain_dog\n",
      "024.Bichon_frise\n",
      "025.Black_and_tan_coonhound\n",
      "026.Black_russian_terrier\n",
      "027.Bloodhound\n",
      "028.Bluetick_coonhound\n",
      "029.Border_collie\n",
      "030.Border_terrier\n",
      "031.Borzoi\n",
      "032.Boston_terrier\n",
      "033.Bouvier_des_flandres\n",
      "034.Boxer\n",
      "035.Boykin_spaniel\n",
      "036.Briard\n",
      "037.Brittany\n",
      "038.Brussels_griffon\n",
      "039.Bull_terrier\n",
      "040.Bulldog\n",
      "041.Bullmastiff\n",
      "042.Cairn_terrier\n",
      "043.Canaan_dog\n",
      "044.Cane_corso\n",
      "045.Cardigan_welsh_corgi\n",
      "046.Cavalier_king_charles_spaniel\n",
      "047.Chesapeake_bay_retriever\n",
      "048.Chihuahua\n",
      "049.Chinese_crested\n",
      "050.Chinese_shar-pei\n",
      "051.Chow_chow\n",
      "052.Clumber_spaniel\n",
      "053.Cocker_spaniel\n",
      "054.Collie\n",
      "055.Curly-coated_retriever\n",
      "056.Dachshund\n",
      "057.Dalmatian\n",
      "058.Dandie_dinmont_terrier\n",
      "059.Doberman_pinscher\n",
      "060.Dogue_de_bordeaux\n",
      "061.English_cocker_spaniel\n",
      "062.English_setter\n",
      "063.English_springer_spaniel\n",
      "064.English_toy_spaniel\n",
      "065.Entlebucher_mountain_dog\n",
      "066.Field_spaniel\n",
      "067.Finnish_spitz\n",
      "068.Flat-coated_retriever\n",
      "069.French_bulldog\n",
      "070.German_pinscher\n",
      "071.German_shepherd_dog\n",
      "072.German_shorthaired_pointer\n",
      "073.German_wirehaired_pointer\n",
      "074.Giant_schnauzer\n",
      "075.Glen_of_imaal_terrier\n",
      "076.Golden_retriever\n",
      "077.Gordon_setter\n",
      "078.Great_dane\n",
      "079.Great_pyrenees\n",
      "080.Greater_swiss_mountain_dog\n",
      "081.Greyhound\n",
      "082.Havanese\n",
      "083.Ibizan_hound\n",
      "084.Icelandic_sheepdog\n",
      "085.Irish_red_and_white_setter\n",
      "086.Irish_setter\n",
      "087.Irish_terrier\n",
      "088.Irish_water_spaniel\n",
      "089.Irish_wolfhound\n",
      "090.Italian_greyhound\n",
      "091.Japanese_chin\n",
      "092.Keeshond\n",
      "093.Kerry_blue_terrier\n",
      "094.Komondor\n",
      "095.Kuvasz\n",
      "096.Labrador_retriever\n",
      "097.Lakeland_terrier\n",
      "098.Leonberger\n",
      "099.Lhasa_apso\n",
      "100.Lowchen\n",
      "101.Maltese\n",
      "102.Manchester_terrier\n",
      "103.Mastiff\n",
      "104.Miniature_schnauzer\n",
      "105.Neapolitan_mastiff\n",
      "106.Newfoundland\n",
      "107.Norfolk_terrier\n",
      "108.Norwegian_buhund\n",
      "109.Norwegian_elkhound\n",
      "110.Norwegian_lundehund\n",
      "111.Norwich_terrier\n",
      "112.Nova_scotia_duck_tolling_retriever\n",
      "113.Old_english_sheepdog\n",
      "114.Otterhound\n",
      "115.Papillon\n",
      "116.Parson_russell_terrier\n",
      "117.Pekingese\n",
      "118.Pembroke_welsh_corgi\n",
      "119.Petit_basset_griffon_vendeen\n",
      "120.Pharaoh_hound\n",
      "121.Plott\n",
      "122.Pointer\n",
      "123.Pomeranian\n",
      "124.Poodle\n",
      "125.Portuguese_water_dog\n",
      "126.Saint_bernard\n",
      "127.Silky_terrier\n",
      "128.Smooth_fox_terrier\n",
      "129.Tibetan_mastiff\n",
      "130.Welsh_springer_spaniel\n",
      "131.Wirehaired_pointing_griffon\n",
      "132.Xoloitzcuintli\n",
      "133.Yorkshire_terrier\n"
     ]
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "n_classes = len(class_names)\n",
    "print(f\"There are {n_classes} classes in the dataset\\n\")\n",
    "\n",
    "for name in class_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Custom CNN model architecture\n",
    "\n",
    "We shall try writing our own custom CNN model instead of directly trying some popular models like VGG, ResNet or DenseNet. While designing, we shall keep in mind, the basic principles which are common in all the CNN models for classification. These are as follows:\n",
    "\n",
    "1. Gradually decrease the size of the activations maps/ outputs.\n",
    "2. Gradually increase the number of filters for Convolutional layer as we go deeper.\n",
    "3. Use ReLU non-linear activation (except in the last classfication layer). \n",
    "4. Also use droput after the FC layers for regularization.\n",
    "5. Use Max-pooling for decreasing the size of the activation maps.\n",
    "6. Use stride=1 as in the case with \"most\" of the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "class CustomNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # use padding such that conv output has size as previous layer, [(Inputâˆ’Kernel+2*padding)/Stride]+1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # (16, 224, 224) --> (16, 112, 112) (halved by max-pool)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # (32, 112, 112) -->  (16, 56, 56) (halved by max-pool) \n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # (64, 56, 56) -- > (64, 28, 28)\n",
    "        self.fc1 = nn.Linear(64*28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        # no of classes `n_classes`: 133\n",
    "        self.fc3 = nn.Linear(256, n_classes)\n",
    "    \n",
    "    # without batch normalisation\n",
    "    def forward(self, x):\n",
    "        # input image: (3, 224, 224)\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        # (16, 112, 112)\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        # (32, 56, 56)\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        # (64, 28, 28)\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = self.dropout(x) # use droput for regularization\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x) # use droput for regularization\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# create the model object\n",
    "model_scratch = CustomNet()\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 224, 224]             448\n",
      "              ReLU-2         [-1, 16, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 16, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]           4,640\n",
      "              ReLU-5         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-6           [-1, 32, 56, 56]               0\n",
      "            Conv2d-7           [-1, 64, 56, 56]          18,496\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "         MaxPool2d-9           [-1, 64, 28, 28]               0\n",
      "          Dropout-10                [-1, 50176]               0\n",
      "           Linear-11                  [-1, 512]      25,690,624\n",
      "             ReLU-12                  [-1, 512]               0\n",
      "          Dropout-13                  [-1, 512]               0\n",
      "           Linear-14                  [-1, 256]         131,328\n",
      "             ReLU-15                  [-1, 256]               0\n",
      "           Linear-16                  [-1, 133]          34,181\n",
      "================================================================\n",
      "Total params: 25,879,717\n",
      "Trainable params: 25,879,717\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 24.52\n",
      "Params size (MB): 98.72\n",
      "Estimated Total Size (MB): 123.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the model summary\n",
    "summary(model_scratch, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a model is much lesser deeper than VGG-16 model as the number of layers having learnable parameters are 6, as compared to the VGG-16 model with 16 layers. A good pratice is to start with a smaller model as \n",
    "deeper models might be tough to train. We will gradually try to increase the layers in the model to see if there is anby performance imporovement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining loss function and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because it is a multi-class classfication task, we use multi-class cross-entropy class\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "# Adam optimiser generally leads to quicker training, and requires lesser tuning as compared to SGD\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        # run the model in training mode\n",
    "        for batch_idx, (data, target) in tqdm(enumerate(loaders['train'])):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            #print(batch_idx)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # find the loss and update the model parameters accordingly\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # record the average training loss\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "        # run the model in evaluation mode\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['val']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # update the average validation loss\n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss,valid_loss))\n",
    "        \n",
    "        # save the model only if validation loss has decreased (save the best model)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f'Validation loss decreased ({valid_loss_min:.3f} ---> {valid_loss:.3f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the custom/scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for 15 epochs\n",
    "model_scratch = train(15, loaders, model_scratch, optimizer_scratch, criterion_scratch, torch.cuda.is_available(), 'custom_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the custom/scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('custom_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %f%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.837285\n",
      "\n",
      "\n",
      "Test Accuracy: 10.807292% (83/768)\n"
     ]
    }
   ],
   "source": [
    "test(loaders, model_scratch, criterion_scratch, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: Our custom model with even very less layers, and hardly trained for few epochs due to computational constraints could achieve near VGG-16 results as mentioned in the reference paper. Although, 10% doesn't seem to be too good of an accuracy, but considering 133 class classificatio  problem, a random classifier would have lesser than 1% accuracy, and 10% now seems to be a good number.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: VGG-16 model with transfer learning\n",
    "\n",
    "We will import pre-built standard CNN classfication models, and will try to compare the accuracy. If time and computation resources allow, we will try training these standard models without pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dogImages/'\n",
    "\n",
    "data_transforms_train =  transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_transforms_valid = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "data_transforms_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# create a dictionary of three Imagefolder objects\n",
    "train_data =  datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms_train)\n",
    "test_data =  datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms_test)\n",
    "val_data =  datasets.ImageFolder(os.path.join(data_dir, 'valid'), data_transforms_valid)\n",
    "\n",
    "# create dataloaders, there is no point in shuffling the testing and validation data\n",
    "loaders = {}\n",
    "loaders['train'] = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)\n",
    "loaders['test'] = DataLoader(test_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "loaders['val'] = DataLoader(val_data, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(Net, self).__init__()\n",
    "        self.pretrained = nn.Sequential(*list(original_model.children())[:-1])\n",
    "\n",
    "        self.finetuned = nn.Sequential(nn.Linear(512*7*7, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(0.2),\n",
    "                           nn.Linear(512, n_classes))\n",
    "#         nn.Sequential(\n",
    "#             nn.Linear(512 * 7 * 7, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(4096, 120),\n",
    "#             nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.finetuned(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "pretrained = vgg16(pretrained=True)\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg16_pretrained = Net(pretrained)\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    vgg16_pretrained.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                  [-1, 512]      12,845,568\n",
      "             ReLU-34                  [-1, 512]               0\n",
      "          Dropout-35                  [-1, 512]               0\n",
      "           Linear-36                  [-1, 133]          68,229\n",
      "================================================================\n",
      "Total params: 27,628,485\n",
      "Trainable params: 12,913,797\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.60\n",
      "Params size (MB): 105.39\n",
      "Estimated Total Size (MB): 324.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the model summary\n",
    "from torchsummary import summary\n",
    "summary(vgg16_pretrained, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that this is much deeper model than our custom model, and it is pretrained on the large ImageNet dataset. We expect that this model already understand the general features of an image. And now we will try to transfer the general knowledge for our dog breed classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.Adam(vgg16_pretrained.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the pre-trained VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:39,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.199297 \tValidation Loss: 1.372656\n",
      "Validation loss decreased (inf ---> 1.373).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:28,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 1.895132 \tValidation Loss: 1.004533\n",
      "Validation loss decreased (1.373 ---> 1.005).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:30,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 1.634808 \tValidation Loss: 0.813026\n",
      "Validation loss decreased (1.005 ---> 0.813).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:26,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 1.471400 \tValidation Loss: 0.785570\n",
      "Validation loss decreased (0.813 ---> 0.786).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:26,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 1.436238 \tValidation Loss: 0.778134\n",
      "Validation loss decreased (0.786 ---> 0.778).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:28,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 1.405385 \tValidation Loss: 0.776769\n",
      "Validation loss decreased (0.778 ---> 0.777).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:30,  1.60s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 1.365881 \tValidation Loss: 0.843558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:29,  1.21it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 1.390453 \tValidation Loss: 0.800375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:31,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 1.376861 \tValidation Loss: 0.767955\n",
      "Validation loss decreased (0.777 ---> 0.768).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:32,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 1.273174 \tValidation Loss: 0.746902\n",
      "Validation loss decreased (0.768 ---> 0.747).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:29,  1.12it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 1.262901 \tValidation Loss: 0.798560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:28,  1.04it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 1.312789 \tValidation Loss: 0.783381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:26,  1.14it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 1.272275 \tValidation Loss: 0.803909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:26,  1.22it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 1.317230 \tValidation Loss: 0.808449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:32,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 1.293354 \tValidation Loss: 0.832512\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        # run the model in training mode\n",
    "        for batch_idx, (data, target) in tqdm(enumerate(loaders['train'])):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            #print(batch_idx)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # find the loss and update the model parameters accordingly\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # record the average training loss\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "        # run the model in evaluation mode\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['val']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # update the average validation loss\n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss,valid_loss))\n",
    "        \n",
    "        # save the model only if validation loss has decreased (save the best model)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f'Validation loss decreased ({valid_loss_min:.3f} ---> {valid_loss:.3f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "vgg16_pretrained = train(15, loaders, vgg16_pretrained, optimizer_transfer, criterion_transfer, torch.cuda.is_available, 'vgg16_pretrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.832465\n",
      "\n",
      "\n",
      "Test Accuracy: 78.365385% (652/832)\n"
     ]
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "vgg16_pretrained.load_state_dict(torch.load('vgg16_pretrained.pt'))\n",
    "\n",
    "def test(loaders, model, criterion, use_cuda):\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %f%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "    \n",
    "test(loaders, vgg16_pretrained, criterion_transfer, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Resnet-101 to compare with pre-trained VGG-16's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-313          [-1, 512, 14, 14]               0\n",
      "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-328            [-1, 512, 7, 7]               0\n",
      "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-331           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
      "          Linear-344                  [-1, 512]       1,049,088\n",
      "            ReLU-345                  [-1, 512]               0\n",
      "         Dropout-346                  [-1, 512]               0\n",
      "          Linear-347                  [-1, 133]          68,229\n",
      "================================================================\n",
      "Total params: 43,617,477\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 42,500,160\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 429.74\n",
      "Params size (MB): 166.39\n",
      "Estimated Total Size (MB): 596.70\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:47,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.274376 \tValidation Loss: 1.223914\n",
      "Validation loss decreased (inf ---> 1.224).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:32,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 1.466539 \tValidation Loss: 0.703403\n",
      "Validation loss decreased (1.224 ---> 0.703).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:28,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 1.192024 \tValidation Loss: 0.547178\n",
      "Validation loss decreased (0.703 ---> 0.547).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:33,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 1.080988 \tValidation Loss: 0.478114\n",
      "Validation loss decreased (0.547 ---> 0.478).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:31,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 1.020919 \tValidation Loss: 0.477737\n",
      "Validation loss decreased (0.478 ---> 0.478).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:30,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 1.004512 \tValidation Loss: 0.437551\n",
      "Validation loss decreased (0.478 ---> 0.438).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:32,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 0.958949 \tValidation Loss: 0.413958\n",
      "Validation loss decreased (0.438 ---> 0.414).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:28,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 0.942190 \tValidation Loss: 0.361269\n",
      "Validation loss decreased (0.414 ---> 0.361).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:27,  1.15it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 0.888914 \tValidation Loss: 0.380626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:42,  1.07s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 0.902732 \tValidation Loss: 0.394522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:30,  1.04it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 0.856674 \tValidation Loss: 0.393285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:27,  1.02it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 0.870481 \tValidation Loss: 0.392891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:30,  1.00it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 0.849959 \tValidation Loss: 0.370500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:29,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 0.845716 \tValidation Loss: 0.353925\n",
      "Validation loss decreased (0.361 ---> 0.354).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [01:36,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 0.827539 \tValidation Loss: 0.353638\n",
      "Validation loss decreased (0.354 ---> 0.354).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "resnet101_pretrained = resnet101(pretrained=True)\n",
    "\n",
    "for param in resnet101_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_ftrs = resnet101_pretrained.fc.in_features\n",
    "\n",
    "classifier = nn.Sequential(nn.Linear(num_ftrs, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(0.2),\n",
    "                           nn.Linear(512, n_classes))\n",
    "resnet101_pretrained.fc = classifier\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    resnet101_pretrained = resnet101_pretrained.cuda()\n",
    "    \n",
    "print(summary(resnet101_pretrained, input_size=(3, 224, 224)))\n",
    "\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.Adam(resnet101_pretrained.fc.parameters(), lr=0.001)\n",
    "\n",
    "resnet101_pretrained = train(15, loaders, resnet101_pretrained, optimizer_transfer, criterion_transfer, torch.cuda.is_available, 'resenet101_pretrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.435837\n",
      "\n",
      "\n",
      "Test Accuracy: 86.538462% (720/832)\n"
     ]
    }
   ],
   "source": [
    "resnet101_pretrained.load_state_dict(torch.load('resenet101_pretrained.pt'))\n",
    "test(loaders, resnet101_pretrained, criterion_transfer, torch.cuda.is_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VGG-16 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.42s/it]\u001b[A\n",
      "2it [00:02,  1.42s/it]\u001b[A\n",
      "3it [00:03,  1.34s/it]\u001b[A\n",
      "4it [00:04,  1.17s/it]\u001b[A\n",
      "5it [00:05,  1.18s/it]\u001b[A\n",
      "6it [00:06,  1.07s/it]\u001b[A\n",
      "7it [00:07,  1.03s/it]\u001b[A\n",
      "8it [00:08,  1.08it/s]\u001b[A\n",
      "9it [00:09,  1.09it/s]\u001b[A\n",
      "10it [00:10,  1.07it/s]\u001b[A\n",
      "11it [00:11,  1.11it/s]\u001b[A\n",
      "12it [00:11,  1.18it/s]\u001b[A\n",
      "13it [00:12,  1.16it/s]\u001b[A\n",
      "14it [00:13,  1.21it/s]\u001b[A\n",
      "15it [00:14,  1.22it/s]\u001b[A\n",
      "16it [00:15,  1.20it/s]\u001b[A\n",
      "17it [00:15,  1.21it/s]\u001b[A\n",
      "18it [00:16,  1.28it/s]\u001b[A\n",
      "19it [00:17,  1.31it/s]\u001b[A\n",
      "20it [00:18,  1.14it/s]\u001b[A\n",
      "21it [00:19,  1.13it/s]\u001b[A\n",
      "22it [00:20,  1.21it/s]\u001b[A\n",
      "23it [00:20,  1.28it/s]\u001b[A\n",
      "24it [00:21,  1.23it/s]\u001b[A\n",
      "25it [00:22,  1.31it/s]\u001b[A\n",
      "26it [00:23,  1.08it/s]\u001b[A\n",
      "27it [00:25,  1.09s/it]\u001b[A\n",
      "28it [00:26,  1.06s/it]\u001b[A\n",
      "29it [00:26,  1.03it/s]\u001b[A\n",
      "30it [00:27,  1.03it/s]\u001b[A\n",
      "31it [00:29,  1.06s/it]\u001b[A\n",
      "32it [00:29,  1.01it/s]\u001b[A\n",
      "33it [00:31,  1.06s/it]\u001b[A\n",
      "34it [00:31,  1.02it/s]\u001b[A\n",
      "35it [00:33,  1.07s/it]\u001b[A\n",
      "36it [00:33,  1.04it/s]\u001b[A\n",
      "37it [00:34,  1.13it/s]\u001b[A\n",
      "38it [00:35,  1.26it/s]\u001b[A\n",
      "39it [00:35,  1.35it/s]\u001b[A\n",
      "40it [00:36,  1.31it/s]\u001b[A\n",
      "41it [00:37,  1.24it/s]\u001b[A\n",
      "42it [00:38,  1.15it/s]\u001b[A\n",
      "43it [00:39,  1.12it/s]\u001b[A\n",
      "44it [00:40,  1.16it/s]\u001b[A\n",
      "45it [00:41,  1.20it/s]\u001b[A\n",
      "46it [00:41,  1.17it/s]\u001b[A\n",
      "47it [00:42,  1.25it/s]\u001b[A\n",
      "48it [00:43,  1.27it/s]\u001b[A\n",
      "49it [00:44,  1.28it/s]\u001b[A\n",
      "50it [00:44,  1.27it/s]\u001b[A\n",
      "51it [00:45,  1.25it/s]\u001b[A\n",
      "52it [00:46,  1.22it/s]\u001b[A\n",
      "53it [00:47,  1.18it/s]\u001b[A\n",
      "54it [00:48,  1.20it/s]\u001b[A\n",
      "55it [00:49,  1.07it/s]\u001b[A\n",
      "56it [00:50,  1.15it/s]\u001b[A\n",
      "57it [00:51,  1.15it/s]\u001b[A\n",
      "58it [00:51,  1.14it/s]\u001b[A\n",
      "59it [00:53,  1.05it/s]\u001b[A\n",
      "60it [00:54,  1.02it/s]\u001b[A\n",
      "61it [00:55,  1.05it/s]\u001b[A\n",
      "62it [00:55,  1.11it/s]\u001b[A\n",
      "63it [00:57,  1.01it/s]\u001b[A\n",
      "64it [00:57,  1.15it/s]\u001b[A\n",
      "65it [00:58,  1.25it/s]\u001b[A\n",
      "66it [00:59,  1.22it/s]\u001b[A\n",
      "67it [01:00,  1.19it/s]\u001b[A\n",
      "68it [01:01,  1.10it/s]\u001b[A\n",
      "69it [01:02,  1.08it/s]\u001b[A\n",
      "70it [01:02,  1.08it/s]\u001b[A\n",
      "71it [01:04,  1.03it/s]\u001b[A\n",
      "72it [01:05,  1.19s/it]\u001b[A\n",
      "73it [01:06,  1.10s/it]\u001b[A\n",
      "74it [01:07,  1.04s/it]\u001b[A\n",
      "75it [01:08,  1.00it/s]\u001b[A\n",
      "76it [01:09,  1.03it/s]\u001b[A\n",
      "77it [01:10,  1.05it/s]\u001b[A\n",
      "78it [01:10,  1.17it/s]\u001b[A\n",
      "79it [01:11,  1.15it/s]\u001b[A\n",
      "80it [01:12,  1.15it/s]\u001b[A\n",
      "81it [01:13,  1.21it/s]\u001b[A\n",
      "82it [01:14,  1.29it/s]\u001b[A\n",
      "83it [01:15,  1.06it/s]\u001b[A\n",
      "84it [01:16,  1.09it/s]\u001b[A\n",
      "85it [01:16,  1.18it/s]\u001b[A\n",
      "86it [01:17,  1.23it/s]\u001b[A\n",
      "87it [01:18,  1.22it/s]\u001b[A\n",
      "88it [01:19,  1.23it/s]\u001b[A\n",
      "89it [01:20,  1.12it/s]\u001b[A\n",
      "90it [01:21,  1.23it/s]\u001b[A\n",
      "91it [01:21,  1.29it/s]\u001b[A\n",
      "92it [01:22,  1.34it/s]\u001b[A\n",
      "93it [01:23,  1.33it/s]\u001b[A\n",
      "94it [01:23,  1.34it/s]\u001b[A\n",
      "95it [01:24,  1.35it/s]\u001b[A\n",
      "96it [01:25,  1.22it/s]\u001b[A\n",
      "97it [01:26,  1.16it/s]\u001b[A\n",
      "98it [01:27,  1.14it/s]\u001b[A\n",
      "99it [01:28,  1.16it/s]\u001b[A\n",
      "100it [01:29,  1.23it/s]\u001b[A\n",
      "101it [01:30,  1.12it/s]\u001b[A\n",
      "102it [01:38,  3.21s/it]\u001b[A\n",
      "103it [01:39,  2.59s/it]\u001b[A\n",
      "104it [01:40,  1.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.912992 \tValidation Loss: 4.871336\n",
      "Validation loss decreased (inf ---> 4.871).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.19it/s]\u001b[A\n",
      "2it [00:01,  1.25it/s]\u001b[A\n",
      "3it [00:02,  1.30it/s]\u001b[A\n",
      "4it [00:03,  1.27it/s]\u001b[A\n",
      "5it [00:03,  1.31it/s]\u001b[A\n",
      "6it [00:04,  1.31it/s]\u001b[A\n",
      "7it [00:05,  1.30it/s]\u001b[A\n",
      "8it [00:06,  1.25it/s]\u001b[A\n",
      "9it [00:07,  1.20it/s]\u001b[A\n",
      "10it [00:07,  1.29it/s]\u001b[A\n",
      "11it [00:08,  1.30it/s]\u001b[A\n",
      "12it [00:09,  1.14it/s]\u001b[A\n",
      "13it [00:10,  1.16it/s]\u001b[A\n",
      "14it [00:11,  1.13it/s]\u001b[A\n",
      "15it [00:12,  1.08it/s]\u001b[A\n",
      "16it [00:13,  1.13it/s]\u001b[A\n",
      "17it [00:13,  1.17it/s]\u001b[A\n",
      "18it [00:14,  1.21it/s]\u001b[A\n",
      "19it [00:16,  1.07s/it]\u001b[A\n",
      "20it [00:17,  1.03it/s]\u001b[A\n",
      "21it [00:17,  1.08it/s]\u001b[A\n",
      "22it [00:19,  1.04it/s]\u001b[A\n",
      "23it [00:19,  1.08it/s]\u001b[A\n",
      "24it [00:20,  1.21it/s]\u001b[A\n",
      "25it [00:21,  1.24it/s]\u001b[A\n",
      "26it [00:22,  1.20it/s]\u001b[A\n",
      "27it [00:22,  1.26it/s]\u001b[A\n",
      "28it [00:23,  1.32it/s]\u001b[A\n",
      "29it [00:24,  1.11it/s]\u001b[A\n",
      "30it [00:25,  1.13it/s]\u001b[A\n",
      "31it [00:26,  1.18it/s]\u001b[A\n",
      "32it [00:27,  1.16it/s]\u001b[A\n",
      "33it [00:27,  1.27it/s]\u001b[A\n",
      "34it [00:28,  1.23it/s]\u001b[A\n",
      "35it [00:29,  1.22it/s]\u001b[A\n",
      "36it [00:30,  1.11it/s]\u001b[A\n",
      "37it [00:31,  1.08it/s]\u001b[A\n",
      "38it [00:32,  1.16it/s]\u001b[A\n",
      "39it [00:33,  1.22it/s]\u001b[A\n",
      "40it [00:34,  1.00s/it]\u001b[A\n",
      "41it [00:35,  1.04s/it]\u001b[A\n",
      "42it [00:36,  1.08it/s]\u001b[A\n",
      "43it [00:36,  1.19it/s]\u001b[A\n",
      "44it [00:37,  1.20it/s]\u001b[A\n",
      "45it [00:38,  1.23it/s]\u001b[A\n",
      "46it [00:39,  1.21it/s]\u001b[A\n",
      "47it [00:40,  1.24it/s]\u001b[A\n",
      "48it [00:43,  1.50s/it]\u001b[A\n",
      "49it [00:43,  1.26s/it]\u001b[A\n",
      "50it [00:44,  1.17s/it]\u001b[A\n",
      "51it [00:45,  1.03it/s]\u001b[A\n",
      "52it [00:46,  1.01it/s]\u001b[A\n",
      "53it [00:47,  1.01it/s]\u001b[A\n",
      "54it [00:48,  1.06it/s]\u001b[A\n",
      "55it [00:48,  1.17it/s]\u001b[A\n",
      "56it [00:49,  1.30it/s]\u001b[A\n",
      "57it [00:50,  1.09it/s]\u001b[A\n",
      "58it [00:51,  1.08it/s]\u001b[A\n",
      "59it [00:52,  1.21it/s]\u001b[A\n",
      "60it [00:53,  1.10it/s]\u001b[A\n",
      "61it [00:54,  1.15it/s]\u001b[A\n",
      "62it [00:54,  1.27it/s]\u001b[A\n",
      "63it [00:55,  1.25it/s]\u001b[A\n",
      "64it [00:56,  1.26it/s]\u001b[A\n",
      "65it [00:56,  1.33it/s]\u001b[A\n",
      "66it [00:57,  1.36it/s]\u001b[A\n",
      "67it [00:58,  1.21it/s]\u001b[A\n",
      "68it [00:59,  1.13it/s]\u001b[A\n",
      "69it [01:00,  1.15it/s]\u001b[A\n",
      "70it [01:01,  1.18it/s]\u001b[A\n",
      "71it [01:02,  1.19it/s]\u001b[A\n",
      "72it [01:02,  1.23it/s]\u001b[A\n",
      "73it [01:03,  1.27it/s]\u001b[A\n",
      "74it [01:04,  1.14it/s]\u001b[A\n",
      "75it [01:05,  1.25it/s]\u001b[A\n",
      "76it [01:06,  1.28it/s]\u001b[A\n",
      "77it [01:07,  1.12it/s]\u001b[A\n",
      "78it [01:08,  1.10it/s]\u001b[A\n",
      "79it [01:09,  1.02s/it]\u001b[A\n",
      "80it [01:10,  1.02s/it]\u001b[A\n",
      "81it [01:11,  1.04s/it]\u001b[A\n",
      "82it [01:12,  1.02s/it]\u001b[A\n",
      "83it [01:13,  1.01s/it]\u001b[A\n",
      "84it [01:14,  1.03it/s]\u001b[A\n",
      "85it [01:15,  1.08it/s]\u001b[A\n",
      "86it [01:16,  1.14it/s]\u001b[A\n",
      "87it [01:17,  1.02it/s]\u001b[A\n",
      "88it [01:18,  1.03it/s]\u001b[A\n",
      "89it [01:19,  1.07it/s]\u001b[A\n",
      "90it [01:20,  1.06it/s]\u001b[A\n",
      "91it [01:20,  1.05it/s]\u001b[A\n",
      "92it [01:21,  1.07it/s]\u001b[A\n",
      "93it [01:22,  1.02it/s]\u001b[A\n",
      "94it [01:23,  1.05it/s]\u001b[A\n",
      "95it [01:24,  1.01it/s]\u001b[A\n",
      "96it [01:25,  1.01s/it]\u001b[A\n",
      "97it [01:26,  1.00it/s]\u001b[A\n",
      "98it [01:28,  1.12s/it]\u001b[A\n",
      "99it [01:29,  1.19s/it]\u001b[A\n",
      "100it [01:30,  1.22s/it]\u001b[A\n",
      "101it [01:31,  1.13s/it]\u001b[A\n",
      "102it [01:33,  1.16s/it]\u001b[A\n",
      "103it [01:34,  1.11s/it]\u001b[A\n",
      "104it [01:35,  1.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 4.858111 \tValidation Loss: 4.824403\n",
      "Validation loss decreased (4.871 ---> 4.824).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.11it/s]\u001b[A\n",
      "2it [00:01,  1.24it/s]\u001b[A\n",
      "3it [00:02,  1.26it/s]\u001b[A\n",
      "4it [00:03,  1.22it/s]\u001b[A\n",
      "5it [00:04,  1.08it/s]\u001b[A\n",
      "6it [00:04,  1.20it/s]\u001b[A\n",
      "7it [00:05,  1.23it/s]\u001b[A\n",
      "8it [00:06,  1.28it/s]\u001b[A\n",
      "9it [00:07,  1.28it/s]\u001b[A\n",
      "10it [00:08,  1.19it/s]\u001b[A\n",
      "11it [00:09,  1.16it/s]\u001b[A\n",
      "12it [00:09,  1.15it/s]\u001b[A\n",
      "13it [00:10,  1.29it/s]\u001b[A\n",
      "14it [00:11,  1.36it/s]\u001b[A\n",
      "15it [00:11,  1.41it/s]\u001b[A\n",
      "16it [00:12,  1.44it/s]\u001b[A\n",
      "17it [00:13,  1.41it/s]\u001b[A\n",
      "18it [00:13,  1.37it/s]\u001b[A\n",
      "19it [00:14,  1.35it/s]\u001b[A\n",
      "20it [00:15,  1.34it/s]\u001b[A\n",
      "21it [00:16,  1.36it/s]\u001b[A\n",
      "22it [00:16,  1.50it/s]\u001b[A\n",
      "23it [00:17,  1.53it/s]\u001b[A\n",
      "24it [00:18,  1.48it/s]\u001b[A\n",
      "25it [00:18,  1.48it/s]\u001b[A\n",
      "26it [00:19,  1.35it/s]\u001b[A\n",
      "27it [00:20,  1.43it/s]\u001b[A\n",
      "28it [00:21,  1.30it/s]\u001b[A\n",
      "29it [00:22,  1.06s/it]\u001b[A\n",
      "30it [00:24,  1.15s/it]\u001b[A\n",
      "31it [00:25,  1.25s/it]\u001b[A\n",
      "32it [00:26,  1.17s/it]\u001b[A\n",
      "33it [00:27,  1.06s/it]\u001b[A\n",
      "34it [00:28,  1.03it/s]\u001b[A\n",
      "35it [00:29,  1.02it/s]\u001b[A\n",
      "36it [00:29,  1.12it/s]\u001b[A\n",
      "37it [00:30,  1.10it/s]\u001b[A\n",
      "38it [00:31,  1.10it/s]\u001b[A\n",
      "39it [00:33,  1.00it/s]\u001b[A\n",
      "40it [00:34,  1.02it/s]\u001b[A\n",
      "41it [00:34,  1.05it/s]\u001b[A\n",
      "42it [00:35,  1.03it/s]\u001b[A\n",
      "43it [00:37,  1.02s/it]\u001b[A\n",
      "44it [00:37,  1.03it/s]\u001b[A\n",
      "45it [00:38,  1.05it/s]\u001b[A\n",
      "46it [00:39,  1.05it/s]\u001b[A\n",
      "47it [00:40,  1.17it/s]\u001b[A\n",
      "48it [00:41,  1.14it/s]\u001b[A\n",
      "49it [00:42,  1.19it/s]\u001b[A\n",
      "50it [00:42,  1.20it/s]\u001b[A\n",
      "51it [00:43,  1.15it/s]\u001b[A\n",
      "52it [00:44,  1.16it/s]\u001b[A\n",
      "53it [00:45,  1.20it/s]\u001b[A\n",
      "54it [00:46,  1.16it/s]\u001b[A\n",
      "55it [00:47,  1.16it/s]\u001b[A\n",
      "56it [00:48,  1.11it/s]\u001b[A\n",
      "57it [00:49,  1.06it/s]\u001b[A\n",
      "58it [00:50,  1.12it/s]\u001b[A\n",
      "59it [00:50,  1.23it/s]\u001b[A\n",
      "60it [00:51,  1.07it/s]\u001b[A\n",
      "61it [00:52,  1.09it/s]\u001b[A\n",
      "62it [00:53,  1.15it/s]\u001b[A\n",
      "63it [00:54,  1.21it/s]\u001b[A\n",
      "64it [00:55,  1.14it/s]\u001b[A\n",
      "65it [00:56,  1.15it/s]\u001b[A\n",
      "66it [00:56,  1.24it/s]\u001b[A\n",
      "67it [00:57,  1.21it/s]\u001b[A\n",
      "68it [00:58,  1.33it/s]\u001b[A\n",
      "69it [00:58,  1.35it/s]\u001b[A\n",
      "70it [00:59,  1.20it/s]\u001b[A\n",
      "71it [01:00,  1.29it/s]\u001b[A\n",
      "72it [01:01,  1.15it/s]\u001b[A\n",
      "73it [01:02,  1.13it/s]\u001b[A\n",
      "74it [01:03,  1.17it/s]\u001b[A\n",
      "75it [01:04,  1.19it/s]\u001b[A\n",
      "76it [01:05,  1.02s/it]\u001b[A\n",
      "77it [01:06,  1.04it/s]\u001b[A\n",
      "78it [01:07,  1.08it/s]\u001b[A\n",
      "79it [01:07,  1.19it/s]\u001b[A\n",
      "80it [01:08,  1.23it/s]\u001b[A\n",
      "81it [01:09,  1.35it/s]\u001b[A\n",
      "82it [01:09,  1.39it/s]\u001b[A\n",
      "83it [01:10,  1.31it/s]\u001b[A\n",
      "84it [01:11,  1.33it/s]\u001b[A\n",
      "85it [01:12,  1.21it/s]\u001b[A\n",
      "86it [01:13,  1.31it/s]\u001b[A\n",
      "87it [01:13,  1.43it/s]\u001b[A\n",
      "88it [01:14,  1.25it/s]\u001b[A\n",
      "89it [01:15,  1.25it/s]\u001b[A\n",
      "90it [01:16,  1.20it/s]\u001b[A\n",
      "91it [01:17,  1.29it/s]\u001b[A\n",
      "92it [01:18,  1.21it/s]\u001b[A\n",
      "93it [01:18,  1.23it/s]\u001b[A\n",
      "94it [01:19,  1.32it/s]\u001b[A\n",
      "95it [01:20,  1.39it/s]\u001b[A\n",
      "96it [01:20,  1.31it/s]\u001b[A\n",
      "97it [01:21,  1.37it/s]\u001b[A\n",
      "98it [01:22,  1.24it/s]\u001b[A\n",
      "99it [01:23,  1.17it/s]\u001b[A\n",
      "100it [01:24,  1.10it/s]\u001b[A\n",
      "101it [01:25,  1.20it/s]\u001b[A\n",
      "102it [01:26,  1.15it/s]\u001b[A\n",
      "103it [01:26,  1.20it/s]\u001b[A\n",
      "104it [01:27,  1.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 4.809144 \tValidation Loss: 4.750602\n",
      "Validation loss decreased (4.824 ---> 4.751).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.20it/s]\u001b[A\n",
      "2it [00:01,  1.10it/s]\u001b[A\n",
      "3it [00:03,  1.03s/it]\u001b[A\n",
      "4it [00:04,  1.03it/s]\u001b[A\n",
      "5it [00:04,  1.07it/s]\u001b[A\n",
      "6it [00:05,  1.09it/s]\u001b[A\n",
      "7it [00:07,  1.03s/it]\u001b[A\n",
      "8it [00:07,  1.10it/s]\u001b[A\n",
      "9it [00:08,  1.18it/s]\u001b[A\n",
      "10it [00:09,  1.17it/s]\u001b[A\n",
      "11it [00:10,  1.14it/s]\u001b[A\n",
      "12it [00:10,  1.19it/s]\u001b[A\n",
      "13it [00:11,  1.23it/s]\u001b[A\n",
      "14it [00:12,  1.19it/s]\u001b[A\n",
      "15it [00:13,  1.19it/s]\u001b[A\n",
      "16it [00:14,  1.23it/s]\u001b[A\n",
      "17it [00:14,  1.32it/s]\u001b[A\n",
      "18it [00:16,  1.06it/s]\u001b[A\n",
      "19it [00:17,  1.02it/s]\u001b[A\n",
      "20it [00:17,  1.15it/s]\u001b[A\n",
      "21it [00:18,  1.27it/s]\u001b[A\n",
      "22it [00:19,  1.21it/s]\u001b[A\n",
      "23it [00:20,  1.24it/s]\u001b[A\n",
      "24it [00:20,  1.30it/s]\u001b[A\n",
      "25it [00:21,  1.33it/s]\u001b[A\n",
      "26it [00:22,  1.44it/s]\u001b[A\n",
      "27it [00:22,  1.49it/s]\u001b[A\n",
      "28it [00:23,  1.36it/s]\u001b[A\n",
      "29it [00:24,  1.32it/s]\u001b[A\n",
      "30it [00:25,  1.37it/s]\u001b[A\n",
      "31it [00:25,  1.32it/s]\u001b[A\n",
      "32it [00:26,  1.33it/s]\u001b[A\n",
      "33it [00:27,  1.35it/s]\u001b[A\n",
      "34it [00:28,  1.36it/s]\u001b[A\n",
      "35it [00:29,  1.24it/s]\u001b[A\n",
      "36it [00:29,  1.40it/s]\u001b[A\n",
      "37it [00:30,  1.31it/s]\u001b[A\n",
      "38it [00:31,  1.21it/s]\u001b[A\n",
      "39it [00:32,  1.31it/s]\u001b[A\n",
      "40it [00:33,  1.18it/s]\u001b[A\n",
      "41it [00:33,  1.23it/s]\u001b[A\n",
      "42it [00:34,  1.20it/s]\u001b[A\n",
      "43it [00:36,  1.04s/it]\u001b[A\n",
      "44it [00:36,  1.05it/s]\u001b[A\n",
      "45it [00:37,  1.12it/s]\u001b[A\n",
      "46it [00:38,  1.22it/s]\u001b[A\n",
      "47it [00:39,  1.24it/s]\u001b[A\n",
      "48it [00:39,  1.22it/s]\u001b[A\n",
      "49it [00:40,  1.19it/s]\u001b[A\n",
      "50it [00:42,  1.08it/s]\u001b[A\n",
      "51it [00:42,  1.09it/s]\u001b[A\n",
      "52it [00:43,  1.16it/s]\u001b[A\n",
      "53it [00:44,  1.28it/s]\u001b[A\n",
      "54it [00:45,  1.24it/s]\u001b[A\n",
      "55it [00:46,  1.19it/s]\u001b[A\n",
      "56it [00:46,  1.14it/s]\u001b[A\n",
      "57it [00:47,  1.19it/s]\u001b[A\n",
      "58it [00:48,  1.24it/s]\u001b[A\n",
      "59it [00:49,  1.29it/s]\u001b[A\n",
      "60it [00:49,  1.35it/s]\u001b[A\n",
      "61it [00:50,  1.31it/s]\u001b[A\n",
      "62it [00:51,  1.16it/s]\u001b[A\n",
      "63it [00:52,  1.18it/s]\u001b[A\n",
      "64it [00:53,  1.20it/s]\u001b[A\n",
      "65it [00:54,  1.12it/s]\u001b[A\n",
      "66it [00:55,  1.02s/it]\u001b[A\n",
      "67it [00:56,  1.05s/it]\u001b[A\n",
      "68it [00:57,  1.03it/s]\u001b[A\n",
      "69it [00:58,  1.16it/s]\u001b[A\n",
      "70it [00:59,  1.07it/s]\u001b[A\n",
      "71it [01:00,  1.09it/s]\u001b[A\n",
      "72it [01:00,  1.17it/s]\u001b[A\n",
      "73it [01:01,  1.21it/s]\u001b[A\n",
      "74it [01:02,  1.11it/s]\u001b[A\n",
      "75it [01:03,  1.05it/s]\u001b[A\n",
      "76it [01:04,  1.13it/s]\u001b[A\n",
      "77it [01:05,  1.10it/s]\u001b[A\n",
      "78it [01:06,  1.09it/s]\u001b[A\n",
      "79it [01:07,  1.08it/s]\u001b[A\n",
      "80it [01:08,  1.03it/s]\u001b[A\n",
      "81it [01:09,  1.09it/s]\u001b[A\n",
      "82it [01:10,  1.08it/s]\u001b[A\n",
      "83it [01:11,  1.12it/s]\u001b[A\n",
      "84it [01:11,  1.16it/s]\u001b[A\n",
      "85it [01:12,  1.08it/s]\u001b[A\n",
      "86it [01:13,  1.14it/s]\u001b[A\n",
      "87it [01:14,  1.24it/s]\u001b[A\n",
      "88it [01:15,  1.24it/s]\u001b[A\n",
      "89it [01:15,  1.34it/s]\u001b[A\n",
      "90it [01:16,  1.39it/s]\u001b[A\n",
      "91it [01:17,  1.35it/s]\u001b[A\n",
      "92it [01:17,  1.31it/s]\u001b[A\n",
      "93it [01:25,  2.65s/it]\u001b[A\n",
      "94it [01:25,  2.05s/it]\u001b[A\n",
      "95it [01:26,  1.72s/it]\u001b[A\n",
      "96it [01:27,  1.53s/it]\u001b[A\n",
      "97it [01:28,  1.28s/it]\u001b[A\n",
      "98it [01:29,  1.14s/it]\u001b[A\n",
      "99it [01:29,  1.03s/it]\u001b[A\n",
      "100it [01:30,  1.02it/s]\u001b[A\n",
      "101it [01:31,  1.04it/s]\u001b[A\n",
      "102it [01:32,  1.03s/it]\u001b[A\n",
      "103it [01:33,  1.01it/s]\u001b[A\n",
      "104it [01:34,  1.01it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 4.776853 \tValidation Loss: 4.708124\n",
      "Validation loss decreased (4.751 ---> 4.708).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.53it/s]\u001b[A\n",
      "2it [00:01,  1.41it/s]\u001b[A\n",
      "3it [00:02,  1.44it/s]\u001b[A\n",
      "4it [00:03,  1.27it/s]\u001b[A\n",
      "5it [00:03,  1.31it/s]\u001b[A\n",
      "6it [00:04,  1.39it/s]\u001b[A\n",
      "7it [00:05,  1.36it/s]\u001b[A\n",
      "8it [00:05,  1.36it/s]\u001b[A\n",
      "9it [00:06,  1.40it/s]\u001b[A\n",
      "10it [00:07,  1.41it/s]\u001b[A\n",
      "11it [00:08,  1.31it/s]\u001b[A\n",
      "12it [00:09,  1.22it/s]\u001b[A\n",
      "13it [00:09,  1.33it/s]\u001b[A\n",
      "14it [00:10,  1.36it/s]\u001b[A\n",
      "15it [00:11,  1.22it/s]\u001b[A\n",
      "16it [00:12,  1.10it/s]\u001b[A\n",
      "17it [00:13,  1.16it/s]\u001b[A\n",
      "18it [00:14,  1.22it/s]\u001b[A\n",
      "19it [00:14,  1.30it/s]\u001b[A\n",
      "20it [00:15,  1.36it/s]\u001b[A\n",
      "21it [00:16,  1.25it/s]\u001b[A\n",
      "22it [00:16,  1.37it/s]\u001b[A\n",
      "23it [00:17,  1.34it/s]\u001b[A\n",
      "24it [00:18,  1.22it/s]\u001b[A\n",
      "25it [00:19,  1.34it/s]\u001b[A\n",
      "26it [00:20,  1.34it/s]\u001b[A\n",
      "27it [00:20,  1.41it/s]\u001b[A\n",
      "28it [00:21,  1.37it/s]\u001b[A\n",
      "29it [00:22,  1.23it/s]\u001b[A\n",
      "30it [00:23,  1.19it/s]\u001b[A\n",
      "31it [00:24,  1.13it/s]\u001b[A\n",
      "32it [00:25,  1.11it/s]\u001b[A\n",
      "33it [00:26,  1.07it/s]\u001b[A\n",
      "34it [00:27,  1.02it/s]\u001b[A\n",
      "35it [00:34,  2.76s/it]\u001b[A\n",
      "36it [00:35,  2.22s/it]\u001b[A\n",
      "37it [00:36,  1.90s/it]\u001b[A\n",
      "38it [00:37,  1.54s/it]\u001b[A\n",
      "39it [00:37,  1.28s/it]\u001b[A\n",
      "40it [00:38,  1.08s/it]\u001b[A\n",
      "41it [00:39,  1.15s/it]\u001b[A\n",
      "42it [00:40,  1.10s/it]\u001b[A\n",
      "43it [00:41,  1.11s/it]\u001b[A\n",
      "44it [00:42,  1.00s/it]\u001b[A\n",
      "45it [00:43,  1.00s/it]\u001b[A\n",
      "46it [00:44,  1.05s/it]\u001b[A\n",
      "47it [00:45,  1.01it/s]\u001b[A\n",
      "48it [00:46,  1.09it/s]\u001b[A\n",
      "49it [00:47,  1.09it/s]\u001b[A\n",
      "50it [00:47,  1.15it/s]\u001b[A\n",
      "51it [00:48,  1.24it/s]\u001b[A\n",
      "52it [00:49,  1.07it/s]\u001b[A\n",
      "53it [00:50,  1.24it/s]\u001b[A\n",
      "54it [00:50,  1.40it/s]\u001b[A\n",
      "55it [00:51,  1.44it/s]\u001b[A\n",
      "56it [00:52,  1.43it/s]\u001b[A\n",
      "57it [00:52,  1.43it/s]\u001b[A\n",
      "58it [00:53,  1.35it/s]\u001b[A\n",
      "59it [00:54,  1.44it/s]\u001b[A\n",
      "60it [00:55,  1.46it/s]\u001b[A\n",
      "61it [00:55,  1.30it/s]\u001b[A\n",
      "62it [00:56,  1.23it/s]\u001b[A\n",
      "63it [00:58,  1.03it/s]\u001b[A\n",
      "64it [00:59,  1.10it/s]\u001b[A\n",
      "65it [01:00,  1.03it/s]\u001b[A\n",
      "66it [01:01,  1.04it/s]\u001b[A\n",
      "67it [01:02,  1.09s/it]\u001b[A\n",
      "68it [01:03,  1.01it/s]\u001b[A\n",
      "69it [01:03,  1.09it/s]\u001b[A\n",
      "70it [01:04,  1.08it/s]\u001b[A\n",
      "71it [01:05,  1.05it/s]\u001b[A\n",
      "72it [01:06,  1.10it/s]\u001b[A\n",
      "73it [01:07,  1.01it/s]\u001b[A\n",
      "74it [01:09,  1.02s/it]\u001b[A\n",
      "75it [01:10,  1.08s/it]\u001b[A\n",
      "76it [01:11,  1.09s/it]\u001b[A\n",
      "77it [01:12,  1.01s/it]\u001b[A\n",
      "78it [01:13,  1.01it/s]\u001b[A\n",
      "79it [01:14,  1.11s/it]\u001b[A\n",
      "80it [01:15,  1.03s/it]\u001b[A\n",
      "81it [01:16,  1.06it/s]\u001b[A\n",
      "82it [01:16,  1.07it/s]\u001b[A\n",
      "83it [01:17,  1.15it/s]\u001b[A\n",
      "84it [01:18,  1.20it/s]\u001b[A\n",
      "85it [01:19,  1.20it/s]\u001b[A\n",
      "86it [01:20,  1.17it/s]\u001b[A\n",
      "87it [01:21,  1.10it/s]\u001b[A\n",
      "88it [01:22,  1.05it/s]\u001b[A\n",
      "89it [01:23,  1.02s/it]\u001b[A\n",
      "90it [01:24,  1.07it/s]\u001b[A\n",
      "91it [01:25,  1.05it/s]\u001b[A\n",
      "92it [01:26,  1.01s/it]\u001b[A\n",
      "93it [01:27,  1.01it/s]\u001b[A\n",
      "94it [01:28,  1.04it/s]\u001b[A\n",
      "95it [01:29,  1.01s/it]\u001b[A\n",
      "96it [01:30,  1.02s/it]\u001b[A\n",
      "97it [01:31,  1.16s/it]\u001b[A\n",
      "98it [01:32,  1.09s/it]\u001b[A\n",
      "99it [01:33,  1.10s/it]\u001b[A\n",
      "100it [01:34,  1.05s/it]\u001b[A\n",
      "101it [01:36,  1.09s/it]\u001b[A\n",
      "102it [01:37,  1.11s/it]\u001b[A\n",
      "103it [01:38,  1.08s/it]\u001b[A\n",
      "104it [01:39,  1.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 4.758449 \tValidation Loss: 4.684705\n",
      "Validation loss decreased (4.708 ---> 4.685).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.65it/s]\u001b[A\n",
      "2it [00:01,  1.67it/s]\u001b[A\n",
      "3it [00:01,  1.78it/s]\u001b[A\n",
      "4it [00:02,  1.56it/s]\u001b[A\n",
      "5it [00:03,  1.53it/s]\u001b[A\n",
      "6it [00:03,  1.46it/s]\u001b[A\n",
      "7it [00:04,  1.43it/s]\u001b[A\n",
      "8it [00:05,  1.42it/s]\u001b[A\n",
      "9it [00:06,  1.37it/s]\u001b[A\n",
      "10it [00:06,  1.41it/s]\u001b[A\n",
      "11it [00:07,  1.42it/s]\u001b[A\n",
      "12it [00:08,  1.37it/s]\u001b[A\n",
      "13it [00:09,  1.39it/s]\u001b[A\n",
      "14it [00:09,  1.32it/s]\u001b[A\n",
      "15it [00:10,  1.30it/s]\u001b[A\n",
      "16it [00:11,  1.38it/s]\u001b[A\n",
      "17it [00:12,  1.22it/s]\u001b[A\n",
      "18it [00:13,  1.20it/s]\u001b[A\n",
      "19it [00:14,  1.10it/s]\u001b[A\n",
      "20it [00:14,  1.19it/s]\u001b[A\n",
      "21it [00:16,  1.08it/s]\u001b[A\n",
      "22it [00:17,  1.07it/s]\u001b[A\n",
      "23it [00:18,  1.05s/it]\u001b[A\n",
      "24it [00:19,  1.02s/it]\u001b[A\n",
      "25it [00:20,  1.06it/s]\u001b[A\n",
      "26it [00:21,  1.00it/s]\u001b[A\n",
      "27it [00:22,  1.02it/s]\u001b[A\n",
      "28it [00:22,  1.12it/s]\u001b[A\n",
      "29it [00:23,  1.18it/s]\u001b[A\n",
      "30it [00:24,  1.17it/s]\u001b[A\n",
      "31it [00:25,  1.06it/s]\u001b[A\n",
      "32it [00:26,  1.04it/s]\u001b[A\n",
      "33it [00:27,  1.12it/s]\u001b[A\n",
      "34it [00:28,  1.03it/s]\u001b[A\n",
      "35it [00:29,  1.01it/s]\u001b[A\n",
      "36it [00:30,  1.08it/s]\u001b[A\n",
      "37it [00:31,  1.12it/s]\u001b[A\n",
      "38it [00:32,  1.09it/s]\u001b[A\n",
      "39it [00:32,  1.13it/s]\u001b[A\n",
      "40it [00:33,  1.16it/s]\u001b[A\n",
      "41it [00:34,  1.21it/s]\u001b[A\n",
      "42it [00:35,  1.11it/s]\u001b[A\n",
      "43it [00:36,  1.09it/s]\u001b[A\n",
      "44it [00:37,  1.12it/s]\u001b[A\n",
      "45it [00:38,  1.08it/s]\u001b[A\n",
      "46it [00:39,  1.04s/it]\u001b[A\n",
      "47it [00:40,  1.01s/it]\u001b[A\n",
      "48it [00:41,  1.04it/s]\u001b[A\n",
      "49it [00:42,  1.13it/s]\u001b[A\n",
      "50it [00:43,  1.10it/s]\u001b[A\n",
      "51it [00:43,  1.14it/s]\u001b[A\n",
      "52it [00:44,  1.17it/s]\u001b[A\n",
      "53it [00:45,  1.29it/s]\u001b[A\n",
      "54it [00:57,  4.21s/it]\u001b[A\n",
      "55it [00:58,  3.24s/it]\u001b[A\n",
      "56it [00:59,  2.60s/it]\u001b[A\n",
      "57it [01:00,  2.13s/it]\u001b[A\n",
      "58it [01:01,  1.78s/it]\u001b[A\n",
      "59it [01:02,  1.49s/it]\u001b[A\n",
      "60it [01:03,  1.29s/it]\u001b[A\n",
      "61it [01:04,  1.28s/it]\u001b[A\n",
      "62it [01:05,  1.17s/it]\u001b[A\n",
      "63it [01:06,  1.09s/it]\u001b[A\n",
      "64it [01:07,  1.03s/it]\u001b[A\n",
      "65it [01:08,  1.00it/s]\u001b[A\n",
      "66it [01:08,  1.09it/s]\u001b[A\n",
      "67it [01:09,  1.14it/s]\u001b[A\n",
      "68it [01:10,  1.13it/s]\u001b[A\n",
      "69it [01:11,  1.01s/it]\u001b[A\n",
      "70it [01:12,  1.09it/s]\u001b[A\n",
      "71it [01:13,  1.08it/s]\u001b[A\n",
      "72it [01:14,  1.03it/s]\u001b[A\n",
      "73it [01:15,  1.07it/s]\u001b[A\n",
      "74it [01:16,  1.07it/s]\u001b[A\n",
      "75it [01:17,  1.05it/s]\u001b[A\n",
      "76it [01:18,  1.11it/s]\u001b[A\n",
      "77it [01:18,  1.14it/s]\u001b[A\n",
      "78it [01:19,  1.18it/s]\u001b[A\n",
      "79it [01:20,  1.24it/s]\u001b[A\n",
      "80it [01:21,  1.18it/s]\u001b[A\n",
      "81it [01:22,  1.08it/s]\u001b[A\n",
      "82it [01:23,  1.08it/s]\u001b[A\n",
      "83it [01:24,  1.01it/s]\u001b[A\n",
      "84it [01:25,  1.09it/s]\u001b[A\n",
      "85it [01:26,  1.03s/it]\u001b[A\n",
      "86it [01:27,  1.00it/s]\u001b[A\n",
      "87it [01:28,  1.02s/it]\u001b[A\n",
      "88it [01:29,  1.08it/s]\u001b[A\n",
      "89it [01:30,  1.08it/s]\u001b[A\n",
      "90it [01:30,  1.17it/s]\u001b[A\n",
      "91it [01:31,  1.13it/s]\u001b[A\n",
      "92it [01:33,  1.03it/s]\u001b[A\n",
      "93it [01:33,  1.13it/s]\u001b[A\n",
      "94it [01:34,  1.23it/s]\u001b[A\n",
      "95it [01:35,  1.27it/s]\u001b[A\n",
      "96it [01:35,  1.33it/s]\u001b[A\n",
      "97it [01:36,  1.23it/s]\u001b[A\n",
      "98it [01:37,  1.08it/s]\u001b[A\n",
      "99it [01:38,  1.05it/s]\u001b[A\n",
      "100it [01:40,  1.05s/it]\u001b[A\n",
      "101it [01:41,  1.01s/it]\u001b[A\n",
      "102it [01:42,  1.03s/it]\u001b[A\n",
      "103it [01:42,  1.04it/s]\u001b[A\n",
      "104it [01:43,  1.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 4.737562 \tValidation Loss: 4.639932\n",
      "Validation loss decreased (4.685 ---> 4.640).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.14it/s]\u001b[A\n",
      "2it [00:01,  1.11it/s]\u001b[A\n",
      "3it [00:02,  1.13it/s]\u001b[A\n",
      "4it [00:03,  1.14it/s]\u001b[A\n",
      "5it [00:04,  1.22it/s]\u001b[A\n",
      "6it [00:04,  1.28it/s]\u001b[A\n",
      "7it [00:06,  1.09it/s]\u001b[A\n",
      "8it [00:06,  1.17it/s]\u001b[A\n",
      "9it [00:07,  1.12it/s]\u001b[A\n",
      "10it [00:08,  1.15it/s]\u001b[A\n",
      "11it [00:09,  1.15it/s]\u001b[A\n",
      "12it [00:10,  1.10it/s]\u001b[A\n",
      "13it [00:11,  1.12it/s]\u001b[A\n",
      "14it [00:12,  1.16it/s]\u001b[A\n",
      "15it [00:13,  1.13it/s]\u001b[A\n",
      "16it [00:14,  1.06it/s]\u001b[A\n",
      "17it [00:15,  1.06s/it]\u001b[A\n",
      "18it [00:16,  1.00it/s]\u001b[A\n",
      "19it [00:17,  1.02s/it]\u001b[A\n",
      "20it [00:18,  1.10it/s]\u001b[A\n",
      "21it [00:18,  1.10it/s]\u001b[A\n",
      "22it [00:19,  1.16it/s]\u001b[A\n",
      "23it [00:20,  1.08it/s]\u001b[A\n",
      "24it [00:22,  1.01s/it]\u001b[A\n",
      "25it [00:22,  1.05it/s]\u001b[A\n",
      "26it [00:23,  1.18it/s]\u001b[A\n",
      "27it [00:24,  1.25it/s]\u001b[A\n",
      "28it [00:25,  1.21it/s]\u001b[A\n",
      "29it [00:25,  1.21it/s]\u001b[A\n",
      "30it [00:26,  1.14it/s]\u001b[A\n",
      "31it [00:27,  1.26it/s]\u001b[A\n",
      "32it [00:28,  1.20it/s]\u001b[A\n",
      "33it [00:29,  1.12it/s]\u001b[A\n",
      "34it [00:30,  1.11it/s]\u001b[A\n",
      "35it [00:31,  1.16it/s]\u001b[A\n",
      "36it [00:32,  1.14s/it]\u001b[A\n",
      "37it [00:33,  1.02s/it]\u001b[A\n",
      "38it [00:34,  1.09it/s]\u001b[A\n",
      "39it [00:35,  1.13it/s]\u001b[A\n",
      "40it [00:35,  1.19it/s]\u001b[A\n",
      "41it [00:36,  1.25it/s]\u001b[A\n",
      "42it [00:37,  1.18it/s]\u001b[A\n",
      "43it [00:38,  1.09it/s]\u001b[A\n",
      "44it [00:39,  1.23it/s]\u001b[A\n",
      "45it [00:40,  1.10it/s]\u001b[A\n",
      "46it [00:41,  1.14it/s]\u001b[A\n",
      "47it [00:41,  1.14it/s]\u001b[A\n",
      "48it [00:42,  1.17it/s]\u001b[A\n",
      "49it [00:43,  1.14it/s]\u001b[A\n",
      "50it [00:44,  1.24it/s]\u001b[A\n",
      "51it [00:45,  1.12it/s]\u001b[A\n",
      "52it [00:46,  1.17it/s]\u001b[A\n",
      "53it [00:46,  1.22it/s]\u001b[A\n",
      "54it [00:47,  1.26it/s]\u001b[A\n",
      "55it [00:48,  1.25it/s]\u001b[A\n",
      "56it [00:49,  1.20it/s]\u001b[A\n",
      "57it [00:50,  1.16it/s]\u001b[A\n",
      "58it [00:50,  1.26it/s]\u001b[A\n",
      "59it [00:51,  1.29it/s]\u001b[A\n",
      "60it [00:52,  1.40it/s]\u001b[A\n",
      "61it [00:53,  1.17it/s]\u001b[A\n",
      "62it [00:54,  1.27it/s]\u001b[A\n",
      "63it [00:54,  1.31it/s]\u001b[A\n",
      "64it [00:55,  1.26it/s]\u001b[A\n",
      "65it [00:56,  1.27it/s]\u001b[A\n",
      "66it [00:57,  1.30it/s]\u001b[A\n",
      "67it [00:57,  1.38it/s]\u001b[A\n",
      "68it [00:58,  1.35it/s]\u001b[A\n",
      "69it [00:59,  1.41it/s]\u001b[A\n",
      "70it [01:00,  1.31it/s]\u001b[A\n",
      "71it [01:00,  1.40it/s]\u001b[A\n",
      "72it [01:01,  1.51it/s]\u001b[A\n",
      "73it [01:02,  1.29it/s]\u001b[A\n",
      "74it [01:03,  1.27it/s]\u001b[A\n",
      "75it [01:03,  1.27it/s]\u001b[A\n",
      "76it [01:04,  1.34it/s]\u001b[A\n",
      "77it [01:05,  1.38it/s]\u001b[A\n",
      "78it [01:05,  1.39it/s]\u001b[A\n",
      "79it [01:06,  1.23it/s]\u001b[A\n",
      "80it [01:08,  1.12it/s]\u001b[A\n",
      "81it [01:08,  1.18it/s]\u001b[A\n",
      "82it [01:09,  1.11it/s]\u001b[A\n",
      "83it [01:10,  1.22it/s]\u001b[A\n",
      "84it [01:11,  1.26it/s]\u001b[A\n",
      "85it [01:11,  1.29it/s]\u001b[A\n",
      "86it [01:12,  1.17it/s]\u001b[A\n",
      "87it [01:13,  1.24it/s]\u001b[A\n",
      "88it [01:14,  1.18it/s]\u001b[A\n",
      "89it [01:15,  1.18it/s]\u001b[A\n",
      "90it [01:16,  1.20it/s]\u001b[A\n",
      "91it [01:16,  1.23it/s]\u001b[A\n",
      "92it [01:18,  1.02it/s]\u001b[A\n",
      "93it [01:19,  1.11it/s]\u001b[A\n",
      "94it [01:19,  1.16it/s]\u001b[A\n",
      "95it [01:20,  1.11it/s]\u001b[A\n",
      "96it [01:21,  1.06it/s]\u001b[A\n",
      "97it [01:22,  1.09it/s]\u001b[A\n",
      "98it [01:24,  1.03s/it]\u001b[A\n",
      "99it [01:24,  1.05it/s]\u001b[A\n",
      "100it [01:25,  1.06it/s]\u001b[A\n",
      "101it [01:26,  1.17it/s]\u001b[A\n",
      "102it [01:27,  1.11it/s]\u001b[A\n",
      "103it [01:28,  1.18it/s]\u001b[A\n",
      "104it [01:28,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 4.723183 \tValidation Loss: 4.635653\n",
      "Validation loss decreased (4.640 ---> 4.636).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.11s/it]\u001b[A\n",
      "2it [00:01,  1.03s/it]\u001b[A\n",
      "3it [00:02,  1.02s/it]\u001b[A\n",
      "4it [00:03,  1.09it/s]\u001b[A\n",
      "5it [00:04,  1.03it/s]\u001b[A\n",
      "6it [00:05,  1.13it/s]\u001b[A\n",
      "7it [00:06,  1.06it/s]\u001b[A\n",
      "8it [00:07,  1.14it/s]\u001b[A\n",
      "9it [00:07,  1.20it/s]\u001b[A\n",
      "10it [00:08,  1.18it/s]\u001b[A\n",
      "11it [00:09,  1.27it/s]\u001b[A\n",
      "12it [00:10,  1.25it/s]\u001b[A\n",
      "13it [00:11,  1.21it/s]\u001b[A\n",
      "14it [00:11,  1.34it/s]\u001b[A\n",
      "15it [00:12,  1.29it/s]\u001b[A\n",
      "16it [00:13,  1.30it/s]\u001b[A\n",
      "17it [00:14,  1.26it/s]\u001b[A\n",
      "18it [00:14,  1.31it/s]\u001b[A\n",
      "19it [00:15,  1.36it/s]\u001b[A\n",
      "20it [00:16,  1.22it/s]\u001b[A\n",
      "21it [00:17,  1.30it/s]\u001b[A\n",
      "22it [00:17,  1.31it/s]\u001b[A\n",
      "23it [00:18,  1.26it/s]\u001b[A\n",
      "24it [00:19,  1.19it/s]\u001b[A\n",
      "25it [00:20,  1.19it/s]\u001b[A\n",
      "26it [00:21,  1.26it/s]\u001b[A\n",
      "27it [00:21,  1.34it/s]\u001b[A\n",
      "28it [00:22,  1.36it/s]\u001b[A\n",
      "29it [00:23,  1.36it/s]\u001b[A\n",
      "30it [00:24,  1.37it/s]\u001b[A\n",
      "31it [00:24,  1.51it/s]\u001b[A\n",
      "32it [00:25,  1.40it/s]\u001b[A\n",
      "33it [00:26,  1.15it/s]\u001b[A\n",
      "34it [00:27,  1.31it/s]\u001b[A\n",
      "35it [00:28,  1.02it/s]\u001b[A\n",
      "36it [00:29,  1.15it/s]\u001b[A\n",
      "37it [00:29,  1.24it/s]\u001b[A\n",
      "38it [00:30,  1.28it/s]\u001b[A\n",
      "39it [00:31,  1.28it/s]\u001b[A\n",
      "40it [00:31,  1.43it/s]\u001b[A\n",
      "41it [00:32,  1.35it/s]\u001b[A\n",
      "42it [00:33,  1.30it/s]\u001b[A\n",
      "43it [00:34,  1.33it/s]\u001b[A\n",
      "44it [00:35,  1.31it/s]\u001b[A\n",
      "45it [00:36,  1.27it/s]\u001b[A\n",
      "46it [00:37,  1.15it/s]\u001b[A\n",
      "47it [00:37,  1.29it/s]\u001b[A\n",
      "48it [00:38,  1.30it/s]\u001b[A\n",
      "49it [00:39,  1.35it/s]\u001b[A\n",
      "50it [00:39,  1.34it/s]\u001b[A\n",
      "51it [00:40,  1.32it/s]\u001b[A\n",
      "52it [00:41,  1.36it/s]\u001b[A\n",
      "53it [00:42,  1.29it/s]\u001b[A\n",
      "54it [00:42,  1.29it/s]\u001b[A\n",
      "55it [00:43,  1.17it/s]\u001b[A\n",
      "56it [00:44,  1.20it/s]\u001b[A\n",
      "57it [00:45,  1.13it/s]\u001b[A\n",
      "58it [00:46,  1.20it/s]\u001b[A\n",
      "59it [00:47,  1.21it/s]\u001b[A\n",
      "60it [00:48,  1.16it/s]\u001b[A\n",
      "61it [00:49,  1.17it/s]\u001b[A\n",
      "62it [00:49,  1.26it/s]\u001b[A\n",
      "63it [00:50,  1.25it/s]\u001b[A\n",
      "64it [00:51,  1.37it/s]\u001b[A\n",
      "65it [00:51,  1.30it/s]\u001b[A\n",
      "66it [00:53,  1.15it/s]\u001b[A\n",
      "67it [00:53,  1.25it/s]\u001b[A\n",
      "68it [00:54,  1.15it/s]\u001b[A\n",
      "69it [00:55,  1.08it/s]\u001b[A\n",
      "70it [00:56,  1.09it/s]\u001b[A\n",
      "71it [00:57,  1.16it/s]\u001b[A\n",
      "72it [00:58,  1.19it/s]\u001b[A\n",
      "73it [00:58,  1.23it/s]\u001b[A\n",
      "74it [00:59,  1.15it/s]\u001b[A\n",
      "75it [01:00,  1.09it/s]\u001b[A\n",
      "76it [01:01,  1.10it/s]\u001b[A\n",
      "77it [01:02,  1.08it/s]\u001b[A\n",
      "78it [01:03,  1.16it/s]\u001b[A\n",
      "79it [01:04,  1.26it/s]\u001b[A\n",
      "80it [01:05,  1.19it/s]\u001b[A\n",
      "81it [01:05,  1.20it/s]\u001b[A\n",
      "82it [01:06,  1.27it/s]\u001b[A\n",
      "83it [01:07,  1.26it/s]\u001b[A\n",
      "84it [01:08,  1.37it/s]\u001b[A\n",
      "85it [01:08,  1.32it/s]\u001b[A\n",
      "86it [01:09,  1.30it/s]\u001b[A\n",
      "87it [01:10,  1.28it/s]\u001b[A\n",
      "88it [01:11,  1.39it/s]\u001b[A\n",
      "89it [01:11,  1.26it/s]\u001b[A\n",
      "90it [01:13,  1.16it/s]\u001b[A\n",
      "91it [01:13,  1.14it/s]\u001b[A\n",
      "92it [01:14,  1.13it/s]\u001b[A\n",
      "93it [01:15,  1.20it/s]\u001b[A\n",
      "94it [01:16,  1.28it/s]\u001b[A\n",
      "95it [01:17,  1.11it/s]\u001b[A\n",
      "96it [01:18,  1.15it/s]\u001b[A\n",
      "97it [01:18,  1.19it/s]\u001b[A\n",
      "98it [01:19,  1.12it/s]\u001b[A\n",
      "99it [01:20,  1.07it/s]\u001b[A\n",
      "100it [01:21,  1.11it/s]\u001b[A\n",
      "101it [01:22,  1.18it/s]\u001b[A\n",
      "102it [01:23,  1.17it/s]\u001b[A\n",
      "103it [01:24,  1.22it/s]\u001b[A\n",
      "104it [01:25,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 4.709727 \tValidation Loss: 4.635141\n",
      "Validation loss decreased (4.636 ---> 4.635).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.16it/s]\u001b[A\n",
      "2it [00:01,  1.24it/s]\u001b[A\n",
      "3it [00:02,  1.28it/s]\u001b[A\n",
      "4it [00:03,  1.22it/s]\u001b[A\n",
      "5it [00:03,  1.28it/s]\u001b[A\n",
      "6it [00:04,  1.35it/s]\u001b[A\n",
      "7it [00:05,  1.39it/s]\u001b[A\n",
      "8it [00:06,  1.24it/s]\u001b[A\n",
      "9it [00:06,  1.28it/s]\u001b[A\n",
      "10it [00:08,  1.11it/s]\u001b[A\n",
      "11it [00:09,  1.05s/it]\u001b[A\n",
      "12it [00:10,  1.02it/s]\u001b[A\n",
      "13it [00:11,  1.06it/s]\u001b[A\n",
      "14it [00:11,  1.11it/s]\u001b[A\n",
      "15it [00:12,  1.09it/s]\u001b[A\n",
      "16it [00:13,  1.07it/s]\u001b[A\n",
      "17it [00:15,  1.01it/s]\u001b[A\n",
      "18it [00:15,  1.10it/s]\u001b[A\n",
      "19it [00:16,  1.08it/s]\u001b[A\n",
      "20it [00:17,  1.10it/s]\u001b[A\n",
      "21it [00:18,  1.13it/s]\u001b[A\n",
      "22it [00:19,  1.14it/s]\u001b[A\n",
      "23it [00:20,  1.12it/s]\u001b[A\n",
      "24it [00:21,  1.03s/it]\u001b[A\n",
      "25it [00:22,  1.07it/s]\u001b[A\n",
      "26it [00:23,  1.01it/s]\u001b[A\n",
      "27it [00:24,  1.01s/it]\u001b[A\n",
      "28it [00:25,  1.04it/s]\u001b[A\n",
      "29it [00:26,  1.04it/s]\u001b[A\n",
      "30it [00:27,  1.02s/it]\u001b[A\n",
      "31it [00:28,  1.07it/s]\u001b[A\n",
      "32it [00:28,  1.14it/s]\u001b[A\n",
      "33it [00:29,  1.09it/s]\u001b[A\n",
      "34it [00:31,  1.04s/it]\u001b[A\n",
      "35it [00:32,  1.03it/s]\u001b[A\n",
      "36it [00:32,  1.07it/s]\u001b[A\n",
      "37it [00:33,  1.10it/s]\u001b[A\n",
      "38it [00:34,  1.11it/s]\u001b[A\n",
      "39it [00:35,  1.10it/s]\u001b[A\n",
      "40it [00:36,  1.09it/s]\u001b[A\n",
      "41it [00:37,  1.01s/it]\u001b[A\n",
      "42it [00:38,  1.09it/s]\u001b[A\n",
      "43it [00:39,  1.07it/s]\u001b[A\n",
      "44it [00:40,  1.13it/s]\u001b[A\n",
      "45it [00:40,  1.22it/s]\u001b[A\n",
      "46it [00:41,  1.38it/s]\u001b[A\n",
      "47it [00:42,  1.35it/s]\u001b[A\n",
      "48it [00:42,  1.35it/s]\u001b[A\n",
      "49it [00:43,  1.33it/s]\u001b[A\n",
      "50it [00:44,  1.23it/s]\u001b[A\n",
      "51it [00:45,  1.17it/s]\u001b[A\n",
      "52it [00:46,  1.15it/s]\u001b[A\n",
      "53it [00:47,  1.17it/s]\u001b[A\n",
      "54it [00:47,  1.25it/s]\u001b[A\n",
      "55it [00:48,  1.19it/s]\u001b[A\n",
      "56it [00:49,  1.18it/s]\u001b[A\n",
      "57it [00:50,  1.17it/s]\u001b[A\n",
      "58it [00:51,  1.21it/s]\u001b[A\n",
      "59it [00:52,  1.24it/s]\u001b[A\n",
      "60it [00:53,  1.19it/s]\u001b[A\n",
      "61it [00:54,  1.01s/it]\u001b[A\n",
      "62it [00:55,  1.01it/s]\u001b[A\n",
      "63it [00:56,  1.00it/s]\u001b[A\n",
      "64it [00:57,  1.01it/s]\u001b[A\n",
      "65it [00:58,  1.10it/s]\u001b[A\n",
      "66it [00:59,  1.10it/s]\u001b[A\n",
      "67it [01:00,  1.05it/s]\u001b[A\n",
      "68it [01:00,  1.08it/s]\u001b[A\n",
      "69it [01:01,  1.10it/s]\u001b[A\n",
      "70it [01:02,  1.08it/s]\u001b[A\n",
      "71it [01:03,  1.07it/s]\u001b[A\n",
      "72it [01:04,  1.08it/s]\u001b[A\n",
      "73it [01:05,  1.12it/s]\u001b[A\n",
      "74it [01:06,  1.04it/s]\u001b[A\n",
      "75it [01:07,  1.04s/it]\u001b[A\n",
      "76it [01:08,  1.05it/s]\u001b[A\n",
      "77it [01:09,  1.10it/s]\u001b[A\n",
      "78it [01:10,  1.02s/it]\u001b[A\n",
      "79it [01:11,  1.06it/s]\u001b[A\n",
      "80it [01:11,  1.19it/s]\u001b[A\n",
      "81it [01:12,  1.23it/s]\u001b[A\n",
      "82it [01:13,  1.23it/s]\u001b[A\n",
      "83it [01:14,  1.24it/s]\u001b[A\n",
      "84it [01:15,  1.15it/s]\u001b[A\n",
      "85it [01:16,  1.19it/s]\u001b[A\n",
      "86it [01:16,  1.21it/s]\u001b[A\n",
      "87it [01:17,  1.35it/s]\u001b[A\n",
      "88it [01:18,  1.33it/s]\u001b[A\n",
      "89it [01:19,  1.22it/s]\u001b[A\n",
      "90it [01:20,  1.20it/s]\u001b[A\n",
      "91it [01:21,  1.10it/s]\u001b[A\n",
      "92it [01:21,  1.21it/s]\u001b[A\n",
      "93it [01:23,  1.02it/s]\u001b[A\n",
      "94it [01:23,  1.14it/s]\u001b[A\n",
      "95it [01:24,  1.16it/s]\u001b[A\n",
      "96it [01:25,  1.15it/s]\u001b[A\n",
      "97it [01:26,  1.14it/s]\u001b[A\n",
      "98it [01:27,  1.19it/s]\u001b[A\n",
      "99it [01:27,  1.22it/s]\u001b[A\n",
      "100it [01:28,  1.15it/s]\u001b[A\n",
      "101it [01:29,  1.17it/s]\u001b[A\n",
      "102it [01:30,  1.12it/s]\u001b[A\n",
      "103it [01:31,  1.24it/s]\u001b[A\n",
      "104it [01:31,  1.36it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 4.706005 \tValidation Loss: 4.582660\n",
      "Validation loss decreased (4.635 ---> 4.583).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.28it/s]\u001b[A\n",
      "2it [00:01,  1.39it/s]\u001b[A\n",
      "3it [00:01,  1.45it/s]\u001b[A\n",
      "4it [00:02,  1.56it/s]\u001b[A\n",
      "5it [00:03,  1.58it/s]\u001b[A\n",
      "6it [00:04,  1.39it/s]\u001b[A\n",
      "7it [00:04,  1.49it/s]\u001b[A\n",
      "8it [00:05,  1.40it/s]\u001b[A\n",
      "9it [00:06,  1.41it/s]\u001b[A\n",
      "10it [00:06,  1.46it/s]\u001b[A\n",
      "11it [00:07,  1.46it/s]\u001b[A\n",
      "12it [00:08,  1.43it/s]\u001b[A\n",
      "13it [00:08,  1.41it/s]\u001b[A\n",
      "14it [00:10,  1.19it/s]\u001b[A\n",
      "15it [00:10,  1.32it/s]\u001b[A\n",
      "16it [00:11,  1.36it/s]\u001b[A\n",
      "17it [00:12,  1.37it/s]\u001b[A\n",
      "18it [00:12,  1.38it/s]\u001b[A\n",
      "19it [00:13,  1.33it/s]\u001b[A\n",
      "20it [00:14,  1.35it/s]\u001b[A\n",
      "21it [00:14,  1.34it/s]\u001b[A\n",
      "22it [00:15,  1.39it/s]\u001b[A\n",
      "23it [00:16,  1.42it/s]\u001b[A\n",
      "24it [00:17,  1.33it/s]\u001b[A\n",
      "25it [00:18,  1.27it/s]\u001b[A\n",
      "26it [00:18,  1.29it/s]\u001b[A\n",
      "27it [00:19,  1.33it/s]\u001b[A\n",
      "28it [00:20,  1.38it/s]\u001b[A\n",
      "29it [00:20,  1.41it/s]\u001b[A\n",
      "30it [00:21,  1.44it/s]\u001b[A\n",
      "31it [00:22,  1.39it/s]\u001b[A\n",
      "32it [00:23,  1.37it/s]\u001b[A\n",
      "33it [00:23,  1.25it/s]\u001b[A\n",
      "34it [00:24,  1.27it/s]\u001b[A\n",
      "35it [00:25,  1.23it/s]\u001b[A\n",
      "36it [00:26,  1.27it/s]\u001b[A\n",
      "37it [00:27,  1.20it/s]\u001b[A\n",
      "38it [00:28,  1.23it/s]\u001b[A\n",
      "39it [00:28,  1.23it/s]\u001b[A\n",
      "40it [00:29,  1.16it/s]\u001b[A\n",
      "41it [00:30,  1.21it/s]\u001b[A\n",
      "42it [00:31,  1.29it/s]\u001b[A\n",
      "43it [00:32,  1.29it/s]\u001b[A\n",
      "44it [00:32,  1.29it/s]\u001b[A\n",
      "45it [00:33,  1.39it/s]\u001b[A\n",
      "46it [00:34,  1.37it/s]\u001b[A\n",
      "47it [00:34,  1.35it/s]\u001b[A\n",
      "48it [00:35,  1.36it/s]\u001b[A\n",
      "49it [00:36,  1.36it/s]\u001b[A\n",
      "50it [00:37,  1.32it/s]\u001b[A\n",
      "51it [00:37,  1.37it/s]\u001b[A\n",
      "52it [00:38,  1.39it/s]\u001b[A\n",
      "53it [00:40,  1.03it/s]\u001b[A\n",
      "54it [00:40,  1.09it/s]\u001b[A\n",
      "55it [00:41,  1.13it/s]\u001b[A\n",
      "56it [00:42,  1.19it/s]\u001b[A\n",
      "57it [00:43,  1.24it/s]\u001b[A\n",
      "58it [00:43,  1.36it/s]\u001b[A\n",
      "59it [00:44,  1.47it/s]\u001b[A\n",
      "60it [00:45,  1.23it/s]\u001b[A\n",
      "61it [00:46,  1.26it/s]\u001b[A\n",
      "62it [00:46,  1.24it/s]\u001b[A\n",
      "63it [00:47,  1.17it/s]\u001b[A\n",
      "64it [00:48,  1.16it/s]\u001b[A\n",
      "65it [00:49,  1.21it/s]\u001b[A\n",
      "66it [00:50,  1.18it/s]\u001b[A\n",
      "67it [00:51,  1.21it/s]\u001b[A\n",
      "68it [00:51,  1.24it/s]\u001b[A\n",
      "69it [00:53,  1.05it/s]\u001b[A\n",
      "70it [00:54,  1.06it/s]\u001b[A\n",
      "71it [00:55,  1.04it/s]\u001b[A\n",
      "72it [00:56,  1.07it/s]\u001b[A\n",
      "73it [00:56,  1.12it/s]\u001b[A\n",
      "74it [00:57,  1.19it/s]\u001b[A\n",
      "75it [00:58,  1.14it/s]\u001b[A\n",
      "76it [00:59,  1.30it/s]\u001b[A\n",
      "77it [01:00,  1.19it/s]\u001b[A\n",
      "78it [01:01,  1.14it/s]\u001b[A\n",
      "79it [01:02,  1.11it/s]\u001b[A\n",
      "80it [01:02,  1.16it/s]\u001b[A\n",
      "81it [01:03,  1.28it/s]\u001b[A\n",
      "82it [01:03,  1.40it/s]\u001b[A\n",
      "83it [01:04,  1.23it/s]\u001b[A\n",
      "84it [01:05,  1.27it/s]\u001b[A\n",
      "85it [01:06,  1.21it/s]\u001b[A\n",
      "86it [01:07,  1.22it/s]\u001b[A\n",
      "87it [01:08,  1.04it/s]\u001b[A\n",
      "88it [01:09,  1.10it/s]\u001b[A\n",
      "89it [01:10,  1.08it/s]\u001b[A\n",
      "90it [01:11,  1.18it/s]\u001b[A\n",
      "91it [01:11,  1.28it/s]\u001b[A\n",
      "92it [01:12,  1.34it/s]\u001b[A\n",
      "93it [01:13,  1.34it/s]\u001b[A\n",
      "94it [01:14,  1.29it/s]\u001b[A\n",
      "95it [01:14,  1.33it/s]\u001b[A\n",
      "96it [01:15,  1.27it/s]\u001b[A\n",
      "97it [01:16,  1.09it/s]\u001b[A\n",
      "98it [01:17,  1.08it/s]\u001b[A\n",
      "99it [01:18,  1.09it/s]\u001b[A\n",
      "100it [01:19,  1.18it/s]\u001b[A\n",
      "101it [01:20,  1.16it/s]\u001b[A\n",
      "102it [01:21,  1.19it/s]\u001b[A\n",
      "103it [01:21,  1.22it/s]\u001b[A\n",
      "104it [01:22,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 4.692046 \tValidation Loss: 4.575372\n",
      "Validation loss decreased (4.583 ---> 4.575).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.11it/s]\u001b[A\n",
      "2it [00:02,  1.02s/it]\u001b[A\n",
      "3it [00:02,  1.08it/s]\u001b[A\n",
      "4it [00:03,  1.10it/s]\u001b[A\n",
      "5it [00:04,  1.11it/s]\u001b[A\n",
      "6it [00:05,  1.20it/s]\u001b[A\n",
      "7it [00:06,  1.15it/s]\u001b[A\n",
      "8it [00:07,  1.11it/s]\u001b[A\n",
      "9it [00:08,  1.11it/s]\u001b[A\n",
      "10it [00:09,  1.06it/s]\u001b[A\n",
      "11it [00:10,  1.09it/s]\u001b[A\n",
      "12it [00:10,  1.19it/s]\u001b[A\n",
      "13it [00:11,  1.22it/s]\u001b[A\n",
      "14it [00:12,  1.23it/s]\u001b[A\n",
      "15it [00:13,  1.26it/s]\u001b[A\n",
      "16it [00:13,  1.21it/s]\u001b[A\n",
      "17it [00:14,  1.24it/s]\u001b[A\n",
      "18it [00:15,  1.31it/s]\u001b[A\n",
      "19it [00:16,  1.33it/s]\u001b[A\n",
      "20it [00:16,  1.30it/s]\u001b[A\n",
      "21it [00:17,  1.27it/s]\u001b[A\n",
      "22it [00:18,  1.19it/s]\u001b[A\n",
      "23it [00:19,  1.19it/s]\u001b[A\n",
      "24it [00:20,  1.11it/s]\u001b[A\n",
      "25it [00:21,  1.12it/s]\u001b[A\n",
      "26it [00:22,  1.02it/s]\u001b[A\n",
      "27it [00:23,  1.13it/s]\u001b[A\n",
      "28it [00:24,  1.14it/s]\u001b[A\n",
      "29it [00:24,  1.18it/s]\u001b[A\n",
      "30it [00:25,  1.17it/s]\u001b[A\n",
      "31it [00:26,  1.17it/s]\u001b[A\n",
      "32it [00:27,  1.27it/s]\u001b[A\n",
      "33it [00:28,  1.22it/s]\u001b[A\n",
      "34it [00:28,  1.23it/s]\u001b[A\n",
      "35it [00:29,  1.30it/s]\u001b[A\n",
      "36it [00:30,  1.36it/s]\u001b[A\n",
      "37it [00:30,  1.45it/s]\u001b[A\n",
      "38it [00:31,  1.26it/s]\u001b[A\n",
      "39it [00:32,  1.32it/s]\u001b[A\n",
      "40it [00:33,  1.38it/s]\u001b[A\n",
      "41it [00:33,  1.38it/s]\u001b[A\n",
      "42it [00:34,  1.35it/s]\u001b[A\n",
      "43it [00:35,  1.34it/s]\u001b[A\n",
      "44it [00:36,  1.31it/s]\u001b[A\n",
      "45it [00:37,  1.26it/s]\u001b[A\n",
      "46it [00:37,  1.25it/s]\u001b[A\n",
      "47it [00:38,  1.29it/s]\u001b[A\n",
      "48it [00:39,  1.35it/s]\u001b[A\n",
      "49it [00:40,  1.39it/s]\u001b[A\n",
      "50it [00:40,  1.29it/s]\u001b[A\n",
      "51it [00:41,  1.29it/s]\u001b[A\n",
      "52it [00:42,  1.11it/s]\u001b[A\n",
      "53it [00:43,  1.13it/s]\u001b[A\n",
      "54it [00:44,  1.06it/s]\u001b[A\n",
      "55it [00:45,  1.06it/s]\u001b[A\n",
      "56it [00:46,  1.03it/s]\u001b[A\n",
      "57it [00:47,  1.01s/it]\u001b[A\n",
      "58it [00:48,  1.11it/s]\u001b[A\n",
      "59it [00:49,  1.02s/it]\u001b[A\n",
      "60it [00:50,  1.06it/s]\u001b[A\n",
      "61it [00:51,  1.05it/s]\u001b[A\n",
      "62it [00:52,  1.11it/s]\u001b[A\n",
      "63it [00:53,  1.14it/s]\u001b[A\n",
      "64it [00:54,  1.16it/s]\u001b[A\n",
      "65it [00:54,  1.26it/s]\u001b[A\n",
      "66it [00:55,  1.17it/s]\u001b[A\n",
      "67it [00:56,  1.21it/s]\u001b[A\n",
      "68it [00:57,  1.30it/s]\u001b[A\n",
      "69it [00:57,  1.32it/s]\u001b[A\n",
      "70it [00:58,  1.29it/s]\u001b[A\n",
      "71it [00:59,  1.33it/s]\u001b[A\n",
      "72it [00:59,  1.40it/s]\u001b[A\n",
      "73it [01:00,  1.43it/s]\u001b[A\n",
      "74it [01:01,  1.37it/s]\u001b[A\n",
      "75it [01:02,  1.28it/s]\u001b[A\n",
      "76it [01:03,  1.25it/s]\u001b[A\n",
      "77it [01:04,  1.20it/s]\u001b[A\n",
      "78it [01:04,  1.19it/s]\u001b[A\n",
      "79it [01:06,  1.01it/s]\u001b[A\n",
      "80it [01:06,  1.09it/s]\u001b[A\n",
      "81it [01:07,  1.08it/s]\u001b[A\n",
      "82it [01:08,  1.08it/s]\u001b[A\n",
      "83it [01:09,  1.03it/s]\u001b[A\n",
      "84it [01:10,  1.06it/s]\u001b[A\n",
      "85it [01:11,  1.02it/s]\u001b[A\n",
      "86it [01:12,  1.09it/s]\u001b[A\n",
      "87it [01:13,  1.02s/it]\u001b[A\n",
      "88it [01:14,  1.01it/s]\u001b[A\n",
      "89it [01:15,  1.03it/s]\u001b[A\n",
      "90it [01:17,  1.05s/it]\u001b[A\n",
      "91it [01:17,  1.03s/it]\u001b[A\n",
      "92it [01:18,  1.03it/s]\u001b[A\n",
      "93it [01:19,  1.06it/s]\u001b[A\n",
      "94it [01:20,  1.17it/s]\u001b[A\n",
      "95it [01:21,  1.13it/s]\u001b[A\n",
      "96it [01:22,  1.04it/s]\u001b[A\n",
      "97it [01:23,  1.09it/s]\u001b[A\n",
      "98it [01:23,  1.19it/s]\u001b[A\n",
      "99it [01:24,  1.12it/s]\u001b[A\n",
      "100it [01:25,  1.09it/s]\u001b[A\n",
      "101it [01:26,  1.08it/s]\u001b[A\n",
      "102it [01:27,  1.15it/s]\u001b[A\n",
      "103it [01:28,  1.15it/s]\u001b[A\n",
      "104it [01:29,  1.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 4.672331 \tValidation Loss: 4.553545\n",
      "Validation loss decreased (4.575 ---> 4.554).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.11s/it]\u001b[A\n",
      "2it [00:01,  1.00s/it]\u001b[A\n",
      "3it [00:02,  1.03s/it]\u001b[A\n",
      "4it [00:03,  1.04it/s]\u001b[A\n",
      "5it [00:04,  1.06it/s]\u001b[A\n",
      "6it [00:05,  1.20it/s]\u001b[A\n",
      "7it [00:05,  1.31it/s]\u001b[A\n",
      "8it [00:06,  1.29it/s]\u001b[A\n",
      "9it [00:07,  1.37it/s]\u001b[A\n",
      "10it [00:07,  1.42it/s]\u001b[A\n",
      "11it [00:09,  1.10it/s]\u001b[A\n",
      "12it [00:10,  1.10it/s]\u001b[A\n",
      "13it [00:11,  1.12it/s]\u001b[A\n",
      "14it [00:12,  1.10it/s]\u001b[A\n",
      "15it [00:13,  1.02it/s]\u001b[A\n",
      "16it [00:14,  1.05it/s]\u001b[A\n",
      "17it [00:14,  1.18it/s]\u001b[A\n",
      "18it [00:15,  1.20it/s]\u001b[A\n",
      "19it [00:16,  1.19it/s]\u001b[A\n",
      "20it [00:17,  1.20it/s]\u001b[A\n",
      "21it [00:17,  1.23it/s]\u001b[A\n",
      "22it [00:19,  1.07it/s]\u001b[A\n",
      "23it [00:20,  1.06it/s]\u001b[A\n",
      "24it [00:21,  1.02it/s]\u001b[A\n",
      "25it [00:22,  1.07s/it]\u001b[A\n",
      "26it [00:23,  1.06s/it]\u001b[A\n",
      "27it [00:24,  1.02s/it]\u001b[A\n",
      "28it [00:25,  1.07it/s]\u001b[A\n",
      "29it [00:26,  1.08it/s]\u001b[A\n",
      "30it [00:26,  1.09it/s]\u001b[A\n",
      "31it [00:27,  1.04it/s]\u001b[A\n",
      "32it [00:29,  1.04s/it]\u001b[A\n",
      "33it [00:30,  1.10s/it]\u001b[A\n",
      "34it [00:31,  1.17s/it]\u001b[A\n",
      "35it [00:32,  1.06s/it]\u001b[A\n",
      "36it [00:33,  1.03s/it]\u001b[A\n",
      "37it [00:34,  1.03s/it]\u001b[A\n",
      "38it [00:35,  1.01it/s]\u001b[A\n",
      "39it [00:36,  1.12it/s]\u001b[A\n",
      "40it [00:36,  1.18it/s]\u001b[A\n",
      "41it [00:38,  1.03s/it]\u001b[A\n",
      "42it [00:39,  1.00it/s]\u001b[A\n",
      "43it [00:40,  1.03it/s]\u001b[A\n",
      "44it [00:40,  1.15it/s]\u001b[A\n",
      "45it [00:41,  1.09it/s]\u001b[A\n",
      "46it [00:42,  1.10it/s]\u001b[A\n",
      "47it [00:43,  1.14it/s]\u001b[A\n",
      "48it [00:44,  1.18it/s]\u001b[A\n",
      "49it [00:45,  1.17it/s]\u001b[A\n",
      "50it [00:45,  1.19it/s]\u001b[A\n",
      "51it [00:46,  1.13it/s]\u001b[A\n",
      "52it [00:47,  1.20it/s]\u001b[A\n",
      "53it [00:48,  1.21it/s]\u001b[A\n",
      "54it [00:49,  1.14it/s]\u001b[A\n",
      "55it [00:50,  1.21it/s]\u001b[A\n",
      "56it [00:51,  1.18it/s]\u001b[A\n",
      "57it [00:52,  1.12it/s]\u001b[A\n",
      "58it [00:53,  1.07it/s]\u001b[A\n",
      "59it [00:53,  1.13it/s]\u001b[A\n",
      "60it [00:54,  1.21it/s]\u001b[A\n",
      "61it [00:55,  1.28it/s]\u001b[A\n",
      "62it [00:56,  1.14it/s]\u001b[A\n",
      "63it [00:57,  1.14it/s]\u001b[A\n",
      "64it [00:57,  1.28it/s]\u001b[A\n",
      "65it [00:58,  1.29it/s]\u001b[A\n",
      "66it [00:59,  1.16it/s]\u001b[A\n",
      "67it [01:00,  1.08it/s]\u001b[A\n",
      "68it [01:01,  1.14it/s]\u001b[A\n",
      "69it [01:02,  1.04s/it]\u001b[A\n",
      "70it [01:04,  1.10s/it]\u001b[A\n",
      "71it [01:04,  1.04it/s]\u001b[A\n",
      "72it [01:06,  1.16s/it]\u001b[A\n",
      "73it [01:07,  1.05s/it]\u001b[A\n",
      "74it [01:08,  1.06s/it]\u001b[A\n",
      "75it [01:09,  1.01s/it]\u001b[A\n",
      "76it [01:09,  1.10it/s]\u001b[A\n",
      "77it [01:10,  1.12it/s]\u001b[A\n",
      "78it [01:11,  1.10it/s]\u001b[A\n",
      "79it [01:12,  1.14it/s]\u001b[A\n",
      "80it [01:13,  1.05it/s]\u001b[A\n",
      "81it [01:14,  1.07s/it]\u001b[A\n",
      "82it [01:15,  1.07s/it]\u001b[A\n",
      "83it [01:16,  1.09it/s]\u001b[A\n",
      "84it [01:17,  1.09it/s]\u001b[A\n",
      "85it [01:18,  1.21it/s]\u001b[A\n",
      "86it [01:18,  1.25it/s]\u001b[A\n",
      "87it [01:19,  1.32it/s]\u001b[A\n",
      "88it [01:20,  1.24it/s]\u001b[A\n",
      "89it [01:21,  1.16it/s]\u001b[A\n",
      "90it [01:22,  1.27it/s]\u001b[A\n",
      "91it [01:22,  1.27it/s]\u001b[A\n",
      "92it [01:23,  1.31it/s]\u001b[A\n",
      "93it [01:24,  1.40it/s]\u001b[A\n",
      "94it [01:24,  1.47it/s]\u001b[A\n",
      "95it [01:25,  1.38it/s]\u001b[A\n",
      "96it [01:26,  1.29it/s]\u001b[A\n",
      "97it [01:27,  1.25it/s]\u001b[A\n",
      "98it [01:28,  1.25it/s]\u001b[A\n",
      "99it [01:28,  1.30it/s]\u001b[A\n",
      "100it [01:29,  1.36it/s]\u001b[A\n",
      "101it [01:30,  1.38it/s]\u001b[A\n",
      "102it [01:31,  1.22it/s]\u001b[A\n",
      "103it [01:31,  1.33it/s]\u001b[A\n",
      "104it [01:32,  1.33it/s]\u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 4.662022 \tValidation Loss: 4.561559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:00,  1.03it/s]\u001b[A\n",
      "2it [00:01,  1.18it/s]\u001b[A\n",
      "3it [00:02,  1.20it/s]\u001b[A\n",
      "4it [00:02,  1.32it/s]\u001b[A\n",
      "5it [00:03,  1.35it/s]\u001b[A\n",
      "6it [00:04,  1.19it/s]\u001b[A\n",
      "7it [00:05,  1.27it/s]\u001b[A\n",
      "8it [00:06,  1.22it/s]\u001b[A\n",
      "9it [00:06,  1.28it/s]\u001b[A\n",
      "10it [00:07,  1.49it/s]\u001b[A\n",
      "11it [00:08,  1.30it/s]\u001b[A\n",
      "12it [00:09,  1.16it/s]\u001b[A\n",
      "13it [00:10,  1.26it/s]\u001b[A\n",
      "14it [00:10,  1.28it/s]\u001b[A\n",
      "15it [00:11,  1.36it/s]\u001b[A\n",
      "16it [00:12,  1.41it/s]\u001b[A\n",
      "17it [00:13,  1.24it/s]\u001b[A\n",
      "18it [00:13,  1.34it/s]\u001b[A\n",
      "19it [00:14,  1.44it/s]\u001b[A\n",
      "20it [00:15,  1.27it/s]\u001b[A\n",
      "21it [00:16,  1.27it/s]\u001b[A\n",
      "22it [00:16,  1.33it/s]\u001b[A\n",
      "23it [00:17,  1.26it/s]\u001b[A\n",
      "24it [00:18,  1.37it/s]\u001b[A\n",
      "25it [00:18,  1.40it/s]\u001b[A\n",
      "26it [00:19,  1.36it/s]\u001b[A\n",
      "27it [00:20,  1.43it/s]\u001b[A\n",
      "28it [00:20,  1.47it/s]\u001b[A\n",
      "29it [00:21,  1.33it/s]\u001b[A\n",
      "30it [00:22,  1.32it/s]\u001b[A\n",
      "31it [00:23,  1.26it/s]\u001b[A\n",
      "32it [00:24,  1.28it/s]\u001b[A\n",
      "33it [00:24,  1.35it/s]\u001b[A\n",
      "34it [00:25,  1.39it/s]\u001b[A\n",
      "35it [00:26,  1.38it/s]\u001b[A\n",
      "36it [00:27,  1.33it/s]\u001b[A\n",
      "37it [00:27,  1.29it/s]\u001b[A\n",
      "38it [00:28,  1.30it/s]\u001b[A\n",
      "39it [00:29,  1.20it/s]\u001b[A\n",
      "40it [00:30,  1.27it/s]\u001b[A\n",
      "41it [00:31,  1.25it/s]\u001b[A\n",
      "42it [00:32,  1.11it/s]\u001b[A\n",
      "43it [00:33,  1.05it/s]\u001b[A\n",
      "44it [00:34,  1.02s/it]\u001b[A\n",
      "45it [00:35,  1.01s/it]\u001b[A\n",
      "46it [00:36,  1.00it/s]\u001b[A\n",
      "47it [00:37,  1.01it/s]\u001b[A\n",
      "48it [00:38,  1.04it/s]\u001b[A\n",
      "49it [00:39,  1.02s/it]\u001b[A\n",
      "50it [00:40,  1.08it/s]\u001b[A\n",
      "51it [00:41,  1.09it/s]\u001b[A\n",
      "52it [00:42,  1.00s/it]\u001b[A\n",
      "53it [00:43,  1.01it/s]\u001b[A\n",
      "54it [00:43,  1.15it/s]\u001b[A\n",
      "55it [00:45,  1.07it/s]\u001b[A\n",
      "56it [00:45,  1.07it/s]\u001b[A\n",
      "57it [00:47,  1.03it/s]\u001b[A\n",
      "58it [00:47,  1.03it/s]\u001b[A\n",
      "59it [00:48,  1.10it/s]\u001b[A\n",
      "60it [00:49,  1.11it/s]\u001b[A\n",
      "61it [00:50,  1.17it/s]\u001b[A\n",
      "62it [00:51,  1.14it/s]\u001b[A\n",
      "63it [00:52,  1.07it/s]\u001b[A\n",
      "64it [00:53,  1.05it/s]\u001b[A\n",
      "65it [00:54,  1.08it/s]\u001b[A\n",
      "66it [00:54,  1.17it/s]\u001b[A\n",
      "67it [00:55,  1.24it/s]\u001b[A\n",
      "68it [00:56,  1.25it/s]\u001b[A\n",
      "69it [00:57,  1.24it/s]\u001b[A\n",
      "70it [00:58,  1.23it/s]\u001b[A\n",
      "71it [01:00,  1.46s/it]\u001b[A\n",
      "72it [01:01,  1.24s/it]\u001b[A\n",
      "73it [01:02,  1.07s/it]\u001b[A\n",
      "74it [01:03,  1.02s/it]\u001b[A\n",
      "75it [01:04,  1.05it/s]\u001b[A\n",
      "76it [01:05,  1.03it/s]\u001b[A\n",
      "77it [01:06,  1.00s/it]\u001b[A\n",
      "78it [01:07,  1.06it/s]\u001b[A\n",
      "79it [01:07,  1.16it/s]\u001b[A\n",
      "80it [01:08,  1.09it/s]\u001b[A\n",
      "81it [01:09,  1.20it/s]\u001b[A\n",
      "82it [01:10,  1.11it/s]\u001b[A\n",
      "83it [01:11,  1.09it/s]\u001b[A\n",
      "84it [01:12,  1.03it/s]\u001b[A\n",
      "85it [01:13,  1.04it/s]\u001b[A\n",
      "86it [01:14,  1.01it/s]\u001b[A\n",
      "87it [01:15,  1.07it/s]\u001b[A\n",
      "88it [01:16,  1.06it/s]\u001b[A\n",
      "89it [01:17,  1.01s/it]\u001b[A\n",
      "90it [01:18,  1.06it/s]\u001b[A\n",
      "91it [01:18,  1.13it/s]\u001b[A\n",
      "92it [01:19,  1.22it/s]\u001b[A\n",
      "93it [01:20,  1.19it/s]\u001b[A\n",
      "94it [01:21,  1.27it/s]\u001b[A\n",
      "95it [01:22,  1.13it/s]\u001b[A\n",
      "96it [01:23,  1.17it/s]\u001b[A\n",
      "97it [01:24,  1.06it/s]\u001b[A\n",
      "98it [01:25,  1.08it/s]\u001b[A\n",
      "99it [01:25,  1.11it/s]\u001b[A\n",
      "100it [01:26,  1.15it/s]\u001b[A\n",
      "101it [01:27,  1.13it/s]\u001b[A\n",
      "102it [01:28,  1.07it/s]\u001b[A\n",
      "103it [01:29,  1.18it/s]\u001b[A\n",
      "104it [01:30,  1.17it/s]\u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 4.650563 \tValidation Loss: 4.566178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:01,  1.20s/it]\u001b[A\n",
      "2it [00:02,  1.15s/it]\u001b[A\n",
      "3it [00:03,  1.07s/it]\u001b[A\n",
      "4it [00:03,  1.03it/s]\u001b[A\n",
      "5it [00:04,  1.09it/s]\u001b[A\n",
      "6it [00:05,  1.11it/s]\u001b[A\n",
      "7it [00:06,  1.08it/s]\u001b[A\n",
      "8it [00:07,  1.02it/s]\u001b[A\n",
      "9it [00:08,  1.04it/s]\u001b[A\n",
      "10it [00:09,  1.11it/s]\u001b[A\n",
      "11it [00:09,  1.20it/s]\u001b[A\n",
      "12it [00:10,  1.24it/s]\u001b[A\n",
      "13it [00:11,  1.21it/s]\u001b[A\n",
      "14it [00:12,  1.25it/s]\u001b[A\n",
      "15it [00:12,  1.32it/s]\u001b[A\n",
      "16it [00:13,  1.27it/s]\u001b[A\n",
      "17it [00:14,  1.18it/s]\u001b[A\n",
      "18it [00:15,  1.24it/s]\u001b[A\n",
      "19it [00:16,  1.23it/s]\u001b[A\n",
      "20it [00:16,  1.33it/s]\u001b[A\n",
      "21it [00:18,  1.09it/s]\u001b[A\n",
      "22it [00:19,  1.09it/s]\u001b[A\n",
      "23it [00:19,  1.18it/s]\u001b[A\n",
      "24it [00:20,  1.16it/s]\u001b[A\n",
      "25it [00:21,  1.10it/s]\u001b[A\n",
      "26it [00:22,  1.18it/s]\u001b[A\n",
      "27it [00:23,  1.13it/s]\u001b[A\n",
      "28it [00:24,  1.18it/s]\u001b[A\n",
      "29it [00:24,  1.26it/s]\u001b[A\n",
      "30it [00:25,  1.15it/s]\u001b[A\n",
      "31it [00:26,  1.11it/s]\u001b[A\n",
      "32it [00:27,  1.12it/s]\u001b[A\n",
      "33it [00:28,  1.20it/s]\u001b[A\n",
      "34it [00:29,  1.03s/it]\u001b[A\n",
      "35it [00:30,  1.03it/s]\u001b[A\n",
      "36it [00:31,  1.11it/s]\u001b[A\n",
      "37it [00:32,  1.24it/s]\u001b[A\n",
      "38it [00:33,  1.17it/s]\u001b[A\n",
      "39it [00:33,  1.27it/s]\u001b[A\n",
      "40it [00:34,  1.24it/s]\u001b[A\n",
      "41it [00:35,  1.22it/s]\u001b[A\n",
      "42it [00:36,  1.22it/s]\u001b[A\n",
      "43it [00:37,  1.23it/s]\u001b[A\n",
      "44it [00:37,  1.19it/s]\u001b[A\n",
      "45it [00:38,  1.26it/s]\u001b[A\n",
      "46it [00:39,  1.20it/s]\u001b[A\n",
      "47it [00:40,  1.29it/s]\u001b[A\n",
      "48it [00:41,  1.14it/s]\u001b[A\n",
      "49it [00:41,  1.27it/s]\u001b[A\n",
      "50it [00:42,  1.27it/s]\u001b[A\n",
      "51it [00:43,  1.16it/s]\u001b[A\n",
      "52it [00:44,  1.18it/s]\u001b[A\n",
      "53it [00:45,  1.13it/s]\u001b[A\n",
      "54it [00:46,  1.12it/s]\u001b[A\n",
      "55it [00:47,  1.23it/s]\u001b[A\n",
      "56it [00:47,  1.22it/s]\u001b[A\n",
      "57it [00:48,  1.17it/s]\u001b[A\n",
      "58it [00:50,  1.02it/s]\u001b[A\n",
      "59it [00:51,  1.01s/it]\u001b[A\n",
      "60it [00:51,  1.10it/s]\u001b[A\n",
      "61it [00:52,  1.11it/s]\u001b[A\n",
      "62it [00:53,  1.17it/s]\u001b[A\n",
      "63it [00:54,  1.13it/s]\u001b[A\n",
      "64it [00:55,  1.17it/s]\u001b[A\n",
      "65it [00:56,  1.07it/s]\u001b[A\n",
      "66it [00:57,  1.08it/s]\u001b[A\n",
      "67it [00:57,  1.19it/s]\u001b[A\n",
      "68it [00:58,  1.23it/s]\u001b[A\n",
      "69it [00:59,  1.09it/s]\u001b[A\n",
      "70it [01:00,  1.05it/s]\u001b[A\n",
      "71it [01:01,  1.02it/s]\u001b[A\n",
      "72it [01:03,  1.07s/it]\u001b[A\n",
      "73it [01:04,  1.02s/it]\u001b[A\n",
      "74it [01:04,  1.10it/s]\u001b[A\n",
      "75it [01:05,  1.08it/s]\u001b[A\n",
      "76it [01:06,  1.13it/s]\u001b[A\n",
      "77it [01:07,  1.21it/s]\u001b[A\n",
      "78it [01:07,  1.25it/s]\u001b[A\n",
      "79it [01:08,  1.22it/s]\u001b[A\n",
      "80it [01:09,  1.13it/s]\u001b[A\n",
      "81it [01:10,  1.15it/s]\u001b[A\n",
      "82it [01:11,  1.16it/s]\u001b[A\n",
      "83it [01:12,  1.18it/s]\u001b[A\n",
      "84it [01:13,  1.13it/s]\u001b[A\n",
      "85it [01:14,  1.10it/s]\u001b[A\n",
      "86it [01:14,  1.16it/s]\u001b[A\n",
      "87it [01:15,  1.13it/s]\u001b[A\n",
      "88it [01:16,  1.14it/s]\u001b[A\n",
      "89it [01:17,  1.13it/s]\u001b[A\n",
      "90it [01:18,  1.07it/s]\u001b[A\n",
      "91it [01:19,  1.12it/s]\u001b[A\n",
      "92it [01:20,  1.07it/s]\u001b[A\n",
      "93it [01:21,  1.05it/s]\u001b[A\n",
      "94it [01:22,  1.09it/s]\u001b[A\n",
      "95it [01:23,  1.08it/s]\u001b[A\n",
      "96it [01:24,  1.01it/s]\u001b[A\n",
      "97it [01:25,  1.09it/s]\u001b[A\n",
      "98it [01:28,  1.71s/it]\u001b[A\n",
      "99it [01:29,  1.53s/it]\u001b[A\n",
      "100it [01:30,  1.39s/it]\u001b[A\n",
      "101it [01:31,  1.30s/it]\u001b[A\n",
      "102it [01:32,  1.17s/it]\u001b[A\n",
      "103it [01:33,  1.16s/it]\u001b[A\n",
      "104it [01:35,  1.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 4.648955 \tValidation Loss: 4.503046\n",
      "Validation loss decreased (4.554 ---> 4.503).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  1.54it/s]\u001b[A\n",
      "2it [00:01,  1.56it/s]\u001b[A\n",
      "3it [00:02,  1.45it/s]\u001b[A\n",
      "4it [00:03,  1.18it/s]\u001b[A\n",
      "5it [00:04,  1.10it/s]\u001b[A\n",
      "6it [00:05,  1.01it/s]\u001b[A\n",
      "7it [00:07,  1.18s/it]\u001b[A\n",
      "8it [00:08,  1.17s/it]\u001b[A\n",
      "9it [00:08,  1.02s/it]\u001b[A\n",
      "10it [00:09,  1.02it/s]\u001b[A\n",
      "11it [00:10,  1.11it/s]\u001b[A\n",
      "12it [00:11,  1.11it/s]\u001b[A\n",
      "13it [00:12,  1.02it/s]\u001b[A\n",
      "14it [00:13,  1.06it/s]\u001b[A\n",
      "15it [00:14,  1.09it/s]\u001b[A\n",
      "16it [00:15,  1.16it/s]\u001b[A\n",
      "17it [00:15,  1.16it/s]\u001b[A\n",
      "18it [00:16,  1.09it/s]\u001b[A\n",
      "19it [00:17,  1.19it/s]\u001b[A\n",
      "20it [00:18,  1.14it/s]\u001b[A\n",
      "21it [00:19,  1.16it/s]\u001b[A\n",
      "22it [00:20,  1.17it/s]\u001b[A\n",
      "23it [00:20,  1.22it/s]\u001b[A\n",
      "24it [00:21,  1.28it/s]\u001b[A\n",
      "25it [00:22,  1.24it/s]\u001b[A\n",
      "26it [00:23,  1.09it/s]\u001b[A\n",
      "27it [00:25,  1.04s/it]\u001b[A\n",
      "28it [00:25,  1.03it/s]\u001b[A\n",
      "29it [00:26,  1.11it/s]\u001b[A\n",
      "30it [00:27,  1.04it/s]\u001b[A\n",
      "31it [00:28,  1.05s/it]\u001b[A\n",
      "32it [00:29,  1.03it/s]\u001b[A\n",
      "33it [00:30,  1.06it/s]\u001b[A\n",
      "34it [00:31,  1.03it/s]\u001b[A\n",
      "35it [00:32,  1.08it/s]\u001b[A\n",
      "36it [00:33,  1.15it/s]\u001b[A\n",
      "37it [00:34,  1.10it/s]\u001b[A\n",
      "38it [00:35,  1.10it/s]\u001b[A\n",
      "39it [00:35,  1.21it/s]\u001b[A\n",
      "40it [00:36,  1.12it/s]\u001b[A\n",
      "41it [00:37,  1.14it/s]\u001b[A\n",
      "42it [00:38,  1.15it/s]\u001b[A\n",
      "43it [00:39,  1.07it/s]\u001b[A\n",
      "44it [00:40,  1.09it/s]\u001b[A\n",
      "45it [00:41,  1.07it/s]\u001b[A\n",
      "46it [00:42,  1.06it/s]\u001b[A\n",
      "47it [00:43,  1.11it/s]\u001b[A\n",
      "48it [00:44,  1.02s/it]\u001b[A\n",
      "49it [00:45,  1.05s/it]\u001b[A\n",
      "50it [00:46,  1.07it/s]\u001b[A\n",
      "51it [00:47,  1.02it/s]\u001b[A\n",
      "52it [00:48,  1.08it/s]\u001b[A\n",
      "53it [00:49,  1.09it/s]\u001b[A\n",
      "54it [00:49,  1.08it/s]\u001b[A\n",
      "55it [00:51,  1.01it/s]\u001b[A\n",
      "56it [00:51,  1.08it/s]\u001b[A\n",
      "57it [00:52,  1.08it/s]\u001b[A\n",
      "58it [00:53,  1.11it/s]\u001b[A\n",
      "59it [00:54,  1.18it/s]\u001b[A\n",
      "60it [00:55,  1.10it/s]\u001b[A\n",
      "61it [00:56,  1.05it/s]\u001b[A\n",
      "62it [00:57,  1.14it/s]\u001b[A\n",
      "63it [00:58,  1.14it/s]\u001b[A\n",
      "64it [00:59,  1.00s/it]\u001b[A\n",
      "65it [01:01,  1.21s/it]\u001b[A\n",
      "66it [01:01,  1.06s/it]\u001b[A\n",
      "67it [01:03,  1.13s/it]\u001b[A\n",
      "68it [01:04,  1.10s/it]\u001b[A\n",
      "69it [01:05,  1.05s/it]\u001b[A\n",
      "70it [01:05,  1.00it/s]\u001b[A\n",
      "71it [01:06,  1.02it/s]\u001b[A\n",
      "72it [01:07,  1.08it/s]\u001b[A\n",
      "73it [01:08,  1.01s/it]\u001b[A\n",
      "74it [01:09,  1.00s/it]\u001b[A\n",
      "75it [01:10,  1.02it/s]\u001b[A\n",
      "76it [01:11,  1.01s/it]\u001b[A\n",
      "77it [01:12,  1.06it/s]\u001b[A\n",
      "78it [01:13,  1.11it/s]\u001b[A\n",
      "79it [01:14,  1.13it/s]\u001b[A\n",
      "80it [01:15,  1.10it/s]\u001b[A\n",
      "81it [01:15,  1.23it/s]\u001b[A\n",
      "82it [01:16,  1.12it/s]\u001b[A\n",
      "83it [01:17,  1.15it/s]\u001b[A\n",
      "84it [01:18,  1.06it/s]\u001b[A\n",
      "85it [01:19,  1.12it/s]\u001b[A\n",
      "86it [01:20,  1.20it/s]\u001b[A\n",
      "87it [01:21,  1.23it/s]\u001b[A\n",
      "88it [01:21,  1.20it/s]\u001b[A\n",
      "89it [01:22,  1.13it/s]\u001b[A\n",
      "90it [01:24,  1.07it/s]\u001b[A\n",
      "91it [01:25,  1.05it/s]\u001b[A\n",
      "92it [01:25,  1.04it/s]\u001b[A\n",
      "93it [01:26,  1.14it/s]\u001b[A\n",
      "94it [01:27,  1.19it/s]\u001b[A\n",
      "95it [01:28,  1.26it/s]\u001b[A\n",
      "96it [01:28,  1.33it/s]\u001b[A\n",
      "97it [01:29,  1.21it/s]\u001b[A\n",
      "98it [01:30,  1.25it/s]\u001b[A\n",
      "99it [01:31,  1.22it/s]\u001b[A\n",
      "100it [01:32,  1.30it/s]\u001b[A\n",
      "101it [01:32,  1.28it/s]\u001b[A\n",
      "102it [01:33,  1.12it/s]\u001b[A\n",
      "103it [01:34,  1.10it/s]\u001b[A\n",
      "104it [01:35,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 4.640911 \tValidation Loss: 4.535429\n"
     ]
    }
   ],
   "source": [
    "no_pretrained = vgg16(pretrained=False)\n",
    "for param in no_pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg16_scratch = Net(no_pretrained)\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    vgg16_scratch.cuda()\n",
    "    \n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.Adam(vgg16_scratch.parameters(), lr=0.001)\n",
    "\n",
    "from tqdm import tqdm\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        # run the model in training mode\n",
    "        for batch_idx, (data, target) in tqdm(enumerate(loaders['train'])):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            #print(batch_idx)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # find the loss and update the model parameters accordingly\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # record the average training loss\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "        # run the model in evaluation mode\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['val']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # update the average validation loss\n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss,valid_loss))\n",
    "        \n",
    "        # save the model only if validation loss has decreased (save the best model)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f'Validation loss decreased ({valid_loss_min:.3f} ---> {valid_loss:.3f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "vgg16_scratch = train(15, loaders, vgg16_scratch, optimizer_transfer, criterion_transfer, torch.cuda.is_available, 'vgg16_scratch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.488107\n",
      "\n",
      "\n",
      "Test Accuracy: 3.846154% (32/832)\n"
     ]
    }
   ],
   "source": [
    "vgg16_scratch.load_state_dict(torch.load('vgg16_scratch.pt'))\n",
    "test(loaders, vgg16_scratch, criterion_transfer, torch.cuda.is_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned eaelier, when there is not too much of data, it can be tough to train a deep model from scratch. Even our custom model which was much smaller than  the VGG-16 model. Whereas, when we took the pre-trained models, even 101 deep layer model could easily be trained. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
